{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJISMr_dT1IG",
    "outputId": "5115df54-4878-472e-9d45-b29d44b9b185"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Requirement already satisfied: pgeocode in /usr/local/lib/python3.9/site-packages (0.3.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (from pgeocode) (1.3.5)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/site-packages (from pgeocode) (1.21.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from pgeocode) (2.26.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/site-packages (from pandas->pgeocode) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/site-packages (from pandas->pgeocode) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests->pgeocode) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/site-packages (from requests->pgeocode) (2.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->pgeocode) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests->pgeocode) (3.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->pgeocode) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pgeocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "r0IZ1p5aTdNd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ws26Qad1G44Q"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "import pgeocode\n",
    "import datetime\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vTNNC9nEG2Hz"
   },
   "outputs": [],
   "source": [
    "def one_hot_feat(feat, feat_list, val):\n",
    "    out_array = np.zeros(len(feat_list))\n",
    "    val = feat+str(val)\n",
    "    pos = np.where(feat_list == val)[0][0]\n",
    "    out_array[pos] = 1\n",
    "\n",
    "    return out_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Uq0objnOTj_X"
   },
   "outputs": [],
   "source": [
    "def get_accept_days(row):\n",
    "    #print(row)\n",
    "    payment_date = datetime.datetime.fromisoformat(row['payment_datetime'])\n",
    "    accept_date = datetime.datetime.fromisoformat(row['acceptance_scan_timestamp'])\n",
    "    days_accept = accept_date - payment_date\n",
    "    return days_accept.days\n",
    "\n",
    "def get_dist(row):\n",
    "    dist = pgeocode.GeoDistance('US')\n",
    "    res = dist.query_postal_code(row['item_zip'], row['buyer_zip'])\n",
    "    if np.isnan(res):\n",
    "        return 10000\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "def get_delivery_days(row):\n",
    "    #print(row)\n",
    "    payment_date = datetime.datetime.fromisoformat(row['payment_datetime'][:10])\n",
    "    delivery_date = datetime.datetime.fromisoformat(row['delivery_date'])\n",
    "    days_delivery = delivery_date - payment_date\n",
    "\n",
    "    if days_delivery.days < 0:\n",
    "        return 0\n",
    "    return days_delivery.days\n",
    "\n",
    "def clean_dec_hand_days(days):\n",
    "    if np.isnan(days):\n",
    "        return -1000\n",
    "    else:\n",
    "        return days\n",
    "\n",
    "def get_zip(zip):\n",
    "    try:\n",
    "        return zip[:5]\n",
    "    except:\n",
    "        return zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "brQm9iayufFC"
   },
   "outputs": [],
   "source": [
    "class EbayDataset(Dataset):\n",
    "    def __init__(self, delivery_df, test_set=0,train_size=300000, quiz_set=0):\n",
    "        data_dir = \"/Users/pamelakatali/Downloads/Ebay_ML/data/\"\n",
    "        \n",
    "        train_df = delivery_df\n",
    "\n",
    "        train_preprocess = pd.DataFrame([])\n",
    "\n",
    "        train_preprocess['accept_days'] = train_df.apply(get_accept_days, axis=1)\n",
    "        train_preprocess['dec_handling_days'] = train_df['declared_handling_days'].apply(clean_dec_hand_days)\n",
    "        train_preprocess['shipping_fee'] = train_df['shipping_fee']\n",
    "\n",
    "        train_preprocess['carrier_min_estimate'] = train_df['carrier_min_estimate']\n",
    "        train_preprocess['carrier_max_estimate'] = train_df['carrier_max_estimate']\n",
    "        train_preprocess['carrier_diff_estimate'] = train_df['carrier_max_estimate'] - train_df['carrier_min_estimate']\n",
    "\n",
    "        dist = pgeocode.GeoDistance('US')\n",
    "        train_df['item_zip'] = train_df['item_zip'].apply(get_zip)\n",
    "        train_df['buyer_zip'] = train_df['buyer_zip'].apply(get_zip)\n",
    "\n",
    "        train_preprocess['shipping_dist'] = dist.query_postal_code(train_df['item_zip'].astype(str).values, train_df['buyer_zip'].astype(str).values)\n",
    "        train_preprocess['shipping_dist'] = train_preprocess['shipping_dist'].fillna(10000)\n",
    "\n",
    "        train_preprocess['item_price'] = train_df['item_price']\n",
    "        train_preprocess['quantity'] = train_df['quantity']\n",
    "\n",
    "        train_preprocess['weight'] = train_df['weight']\n",
    "        train_preprocess['weight_units'] = train_df['weight_units']\n",
    "\n",
    "        self.cols = list(train_preprocess.columns)\n",
    "\n",
    "\n",
    "        train_preprocess['ship_id'] = train_df['shipment_method_id']\n",
    "        train_preprocess['cat_id'] = train_df['category_id']\n",
    "        train_preprocess['pack_size'] = train_df['package_size']\n",
    "        train_preprocess['b2c_c2c'] = train_df['b2c_c2c']\n",
    "\n",
    "\n",
    "        self.ship_cols = pickle.load( open( data_dir+\"ship_id_cols.pkl\", \"rb\" ) )\n",
    "        self.cat_cols = pickle.load( open( data_dir+\"cat_id_cols.pkl\", \"rb\" ) )\n",
    "        self.pack_cols = pickle.load( open( data_dir+\"pack_size_cols.pkl\", \"rb\" ) )\n",
    "        self.b2c_cols = pickle.load( open( data_dir+\"b2c_c2c_cols.pkl\", \"rb\" ) )\n",
    "\n",
    "        scaler = pickle.load( open( data_dir+\"train_minmax_scaler_2.pkl\", \"rb\" ) )\n",
    "\n",
    "    \n",
    "        if quiz_set == 0:\n",
    "            train_preprocess['delivery_days'] = train_df.apply(get_delivery_days, axis=1)\n",
    "            self.data = train_preprocess\n",
    "\n",
    "        else:\n",
    "            train_preprocess['delivery_days'] = np.zeros(len(train_preprocess))\n",
    "            self.data = train_preprocess\n",
    "  \n",
    "        self.scaler = pickle.load(open( data_dir+\"train_minmax_scaler_2.pkl\", \"rb\" ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data.iloc[idx]\n",
    "        #print(sample)\n",
    "\n",
    "        ship = one_hot_feat('ship_id_', self.ship_cols, sample['ship_id'])\n",
    "        cat = one_hot_feat('cat_id_', self.cat_cols, sample['cat_id'])\n",
    "        pack = one_hot_feat('pack_size_', self.pack_cols, sample['pack_size'])\n",
    "        b2c_c2c = one_hot_feat('b2c_c2c_', self.b2c_cols, sample['b2c_c2c'])\n",
    "\n",
    "        one_hot = np.concatenate((ship, cat, pack, b2c_c2c), axis=0)\n",
    "\n",
    "        x = np.concatenate((sample[self.cols].values.astype(np.float64), one_hot), axis=0)\n",
    "        x = self.scaler.transform(x.reshape(1,78))\n",
    "        x = x.reshape(78,)\n",
    "        return x, sample['delivery_days'] #torch.tensor(x), torch.tensor(sample['delivery_days']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-LZBfrpXTqrf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.w1 = nn.Linear(78,512,bias=True)\n",
    "        nn.init.xavier_uniform_(self.w1.weight)\n",
    "        nn.init.zeros_(self.w1.bias)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(512)\n",
    "        \n",
    "\n",
    "        self.w2 = nn.Linear(512,256,bias=True)\n",
    "        nn.init.xavier_uniform_(self.w2.weight)\n",
    "        nn.init.zeros_(self.w2.bias)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(256)\n",
    "        #nn.Dropout(0.20)\n",
    "        \n",
    "        self.w3 = nn.Linear(256,128,bias=True)\n",
    "        nn.init.xavier_uniform_(self.w3.weight)\n",
    "        nn.init.zeros_(self.w3.bias)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.w4 = nn.Linear(128,64,bias=True)\n",
    "        nn.init.xavier_uniform_(self.w4.weight)\n",
    "        nn.init.zeros_(self.w4.bias)\n",
    "        self.bn4 = torch.nn.BatchNorm1d(64)\n",
    "        #nn.Dropout(0.20)\n",
    "        \n",
    "        self.w5 = nn.Linear(64,32,bias=True)\n",
    "        nn.init.xavier_uniform_(self.w5.weight)\n",
    "        nn.init.zeros_(self.w5.bias)\n",
    "        self.bn5 = torch.nn.BatchNorm1d(32)\n",
    "        \n",
    "        self.w6 = nn.Linear(32,16,bias=True)\n",
    "        nn.init.xavier_uniform_(self.w6.weight)\n",
    "        nn.init.zeros_(self.w6.bias)\n",
    "        self.bn6 = torch.nn.BatchNorm1d(16)\n",
    "        \n",
    "        self.w7 = nn.Linear(16,8,bias=True)\n",
    "        nn.init.xavier_uniform_(self.w7.weight)\n",
    "        nn.init.zeros_(self.w7.bias)\n",
    "        self.bn7 = torch.nn.BatchNorm1d(8)\n",
    "\n",
    "        self.w8 = nn.Linear(8,1,bias=True)\n",
    "        nn.init.xavier_uniform_(self.w8.weight)\n",
    "        nn.init.zeros_(self.w8.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(F.relu(self.w1(x)))\n",
    "        x = self.bn2(F.relu(self.w2(x)))\n",
    "        x = self.bn3(F.relu(self.w3(x)))\n",
    "        x = self.bn4(F.relu(self.w4(x)))\n",
    "        x = self.bn5(F.relu(self.w5(x)))\n",
    "        x = self.bn6(F.relu(self.w6(x)))\n",
    "        x = self.bn7(F.relu(self.w7(x)))\n",
    "        x = self.w8(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exJLdHrbT_Fe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jyHg1rtGT8OQ",
    "outputId": "1bf4592c-90cb-4ff6-a2d6-64455cf64226"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (w1): Linear(in_features=78, out_features=512, bias=True)\n",
       "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (w2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (w3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (w4): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (w5): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (bn5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (w6): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (bn6): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (w7): Linear(in_features=16, out_features=8, bias=True)\n",
       "  (bn7): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (w8): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP()\n",
    "model.load_state_dict(torch.load(\"/Users/pamelakatali/Downloads/Ebay_ML/data/mlp_small/mlp_mid_train_0_epoch.pt\",map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-H-Q_FAiUDWp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "data_dir = \"/Users/pamelakatali/Downloads/Ebay_ML/data/\"\n",
    "\n",
    "chunk_size = 50000\n",
    "quiz_filename = data_dir+\"eBay_ML_Challenge_Dataset_2021_quiz.tsv.gz\"\n",
    "\n",
    "current_loss = 0.0\n",
    "\n",
    "targets_all = torch.tensor([])\n",
    "outputs_all = torch.tensor([])\n",
    "\n",
    "for chunk in pd.read_csv(quiz_filename, sep='\\t', chunksize=chunk_size):\n",
    "    #process chunk\n",
    "    dataset = EbayDataset(chunk, train_size=chunk_size, quiz_set=1)\n",
    "    \n",
    "    # Prepare dataset\n",
    "    quizloader = torch.utils.data.DataLoader(dataset, batch_size=10000, shuffle=True, num_workers=0)\n",
    "    \n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(quizloader):\n",
    "        # Get inputs\n",
    "        inputs, targets = data\n",
    "\n",
    "        outputs = model(inputs.float())\n",
    "        outputs_all = torch.cat((outputs_all, outputs), 0)\n",
    "        targets_all = torch.cat((targets_all, targets), 0)\n",
    "        #break;\n",
    "    #break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abfqp5fqvR_l",
    "outputId": "44f36875-b8e5-4bbe-c68b-1275e8a8c5f8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "UK6Anlc4UPZ5"
   },
   "outputs": [],
   "source": [
    "#inputs_all, targets_all = next(iter(quiz_loader))\n",
    "#outputs_all = model(inputs_all.float())\n",
    "\n",
    "#y_pred = sftmx(outputs_all.detach()) #np.round(outputs.detach().numpy())\n",
    "#y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "y_pred = outputs_all.detach()\n",
    "y_pred = np.round(y_pred.numpy().reshape(len(y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2VpUV5z3UbtO",
    "outputId": "c2a86ee7-86f7-459d-eee9-d382a69ab17c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 4., 4., ..., 5., 4., 4.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3jRw6pnuUa0D"
   },
   "outputs": [],
   "source": [
    "dataset_quiz = None\n",
    "quiz_loader = None\n",
    "inputs_all = None\n",
    "outputs_all = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "bkZRCtWAUiD2"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def get_delivery_date_quiz(row):\n",
    "  \n",
    "    payment_date = datetime.datetime.fromisoformat(row['payment_datetime'][:10])\n",
    "    #print(payment_date)\n",
    "    #delivery_date = datetime.fromisoformat(row['delivery_date'])\n",
    "    delivery_date = payment_date + datetime.timedelta(days=row['mlp_pred'])\n",
    "    return delivery_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "dL1LT0GzUkvY"
   },
   "outputs": [],
   "source": [
    "quiz_df = pd.read_csv(data_dir+\"eBay_ML_Challenge_Dataset_2021_quiz.tsv.gz\", sep=\"\\t\")\n",
    "quiz_df['mlp_pred'] = y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "lgGGINVsUm-W"
   },
   "outputs": [],
   "source": [
    "quiz_df['mlp_pred'] = quiz_df.apply(get_delivery_date_quiz, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "WXsR42EoUnld"
   },
   "outputs": [],
   "source": [
    "quiz_df[['record_number', 'mlp_pred']].to_csv(data_dir+'eBay_ML_Challenge_Dataset_2021_quiz_mlp_pred.tsv.gz',sep='\\t',header=False, index=False, compression='infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsfJbZxYVTyH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dE8sUXO2Vq0-"
   },
   "outputs": [],
   "source": [
    "Chunk: 2440\n",
    "Mini-batch: 48800\n",
    "Train loss: 0.794\n",
    "Test loss: 0.8074281215667725"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbv16sdzhOM1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjlDzFNHhOJq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74C7F4zBhOGR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbOZ6Jd0hOCt"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "train_set = pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/Ebay/data/eBay_ML_Challenge_Dataset_2021_train.tsv.gz\",sep='\\t')\n",
    "train_set\n",
    "\n",
    "train_set = train_set.sample(frac=1)\n",
    "\n",
    "train_len = int(len(train_set) * 0.9)\n",
    "test_set = train_set[train_len:]\n",
    "train_set = train_set[:train_len]\n",
    "\n",
    "test_set.to_csv('/content/drive/MyDrive/Colab_Notebooks/Ebay/data/ebay_dev.tsv.gz',sep='\\t',compression='infer')\n",
    "train_set.to_csv('/content/drive/MyDrive/Colab_Notebooks/Ebay/data/ebay_train.tsv.gz',sep='\\t',compression='infer')\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "predict.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
