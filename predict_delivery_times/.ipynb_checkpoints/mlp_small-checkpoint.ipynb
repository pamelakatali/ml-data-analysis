{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FLu0XefAi_-f",
    "outputId": "f131cc3e-03ea-4227-91ad-01d4409eac28"
   },
   "outputs": [],
   "source": [
    "#!pip3 install pgeocode\n",
    "pgeocode\n",
    "torch\n",
    "torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dvm2EK7strc_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "import pgeocode\n",
    "import datetime\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import EbayDataset\n",
    "import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/pamelakatali/Downloads/Ebay_ML/data/'\n",
    "#data_dir = '/content/drive/MyDrive/Colab_Notebooks/Ebay/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Wvxc4_DgPncu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_QRlNRKwZI1n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "F9flotFRNIYr"
   },
   "outputs": [],
   "source": [
    "#train_temp = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/Ebay/data/ebay_train.tsv.gz', sep='\\t', nrows=3000000)\n",
    "#train_temp.to_csv('/content/drive/MyDrive/Colab_Notebooks/Ebay/data/ebay_train_small.tsv.gz', sep='\\t')\n",
    "#del train_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "L8ZVyr3ReZiI"
   },
   "outputs": [],
   "source": [
    "def shuffle_train_set():\n",
    "\n",
    "    #print(\"Train set\")\n",
    "    train_chunk = pd.read_csv(data_dir+'ebay_train_small.tsv.gz', sep='\\t')\n",
    "    train_chunk = train_chunk.sample(frac=1)\n",
    "    shuffle_lst = train_chunk.index.tolist()\n",
    "    train_chunk.to_csv(data_dir+'ebay_train_small_shuffle.tsv.gz', sep='\\t')\n",
    "    train_chunk = None\n",
    "    del train_chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yowwpUwGER9j",
    "outputId": "24494c96-fe33-41b6-b968-9489caf00512"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Shuffling data\n",
      "Start training\n",
      "Chunk: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.7_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.7_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'EbayDataset' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set fixed random number seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Initialize the MLP\n",
    "mlp = MLP()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
    "\n",
    "sftmx = nn.Softmax(dim=1)\n",
    "\n",
    "BATCH_SIZE = 4096\n",
    "CHUNK_SIZE = 1000000\n",
    "TEST_CHUNK_SIZE = 100000\n",
    "EPOCHS = 20\n",
    "EVAL_BATCH = 120\n",
    "\n",
    "# Run the training loop\n",
    "for epoch in range(0, EPOCHS): # 5 epochs at maximum\n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "    # Shuffle data\n",
    "    print(\"Shuffling data\")\n",
    "    #shuffle_train_set()\n",
    "\n",
    "    train_filename = data_dir+\"ebay_train_small_shuffle.tsv.gz\"\n",
    "    test_filename = data_dir+\"ebay_dev.tsv.gz\"\n",
    "\n",
    "    current_loss = 0.0\n",
    "    print(\"Start training\")\n",
    "    \n",
    "    chunk_count = 0\n",
    "    for chunk in pd.read_csv(train_filename, sep='\\t', chunksize=CHUNK_SIZE):\n",
    "        print(\"Chunk:\",chunk_count)\n",
    "        chunk_count += 1\n",
    "        #process chunk\n",
    "        dataset = EbayDataset(chunk, train_size=CHUNK_SIZE)\n",
    "        # Prepare dataset\n",
    "        trainloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)\n",
    "    \n",
    "        # Iterate over the DataLoader for training data\n",
    "        for i, data in enumerate(trainloader):\n",
    "            mlp.train()\n",
    "\n",
    "            # Get inputs\n",
    "            inputs, targets = data\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Perform forward pass\n",
    "            outputs = mlp(inputs.float())\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_function(outputs, torch.reshape(targets.float(),(len(outputs),1)))\n",
    "\n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print statistics\n",
    "            current_loss += loss.item()\n",
    "\n",
    "\n",
    "            if i % EVAL_BATCH == (EVAL_BATCH - 1):\n",
    "                print('Loss after mini-batch %5d: %.3f' %\n",
    "                        (i + 1, current_loss / EVAL_BATCH))\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    test_chunk = pd.read_csv(test_filename, sep='\\t', nrows=TEST_CHUNK_SIZE)\n",
    "\n",
    "                    #process chunk\n",
    "                    test_dataset = EbayDataset(test_chunk, train_size=TEST_CHUNK_SIZE)\n",
    "                    # Prepare dataset\n",
    "                    testloader = torch.utils.data.DataLoader(test_dataset, batch_size=TEST_CHUNK_SIZE, shuffle=False, num_workers=1)\n",
    "\n",
    "                    targets_all = torch.tensor([])\n",
    "                    outputs_all = torch.tensor([])\n",
    "\n",
    "                    for i, data in enumerate(testloader):\n",
    "                        inputs, targets = data\n",
    "\n",
    "                        outputs = mlp(inputs.float())\n",
    "                        outputs_all = torch.cat((outputs_all, outputs), 0)\n",
    "                        targets_all = torch.cat((targets_all, targets), 0)\n",
    "\n",
    "\n",
    "                    loss_test = loss_function(outputs_all, torch.reshape(targets_all.float(),(len(outputs_all),1)))\n",
    "                    y_pred = sftmx(outputs_all.detach())\n",
    "                    y_pred = np.argmax(y_pred, axis=1)\n",
    "                    acc = accuracy_score(targets_all, y_pred)\n",
    "                    print(\"Test loss:\", loss_test.item())\n",
    "                    print(\"Test accuracy:\", acc)\n",
    "\n",
    "                    current_loss = 0.0\n",
    "    torch.save(mlp.state_dict(), data_dir+\"mlp_small/mlp_mid_train\"+str(epoch)+\"_epoch.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dILc50Z8EuwZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJc0zkQjQC-a"
   },
   "outputs": [],
   "source": [
    "#use ebay critereon \n",
    "#120+ epochs\n",
    "#learning rate or 0.0001\n",
    "#batch size = 256\n",
    "#512 -> 256 -> 128 -> 64 -> 8 - > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WecWTx1iK1Gf",
    "outputId": "1a0fa81e-7426-4db6-e6c6-cc43e8017dd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Loss after mini-batch    50: 31.562\n",
      "Loss after mini-batch   100: 29.200\n",
      "Loss after mini-batch   150: 26.045\n",
      "Loss after mini-batch   200: 21.559\n",
      "Loss after mini-batch    50: 13.221\n",
      "Loss after mini-batch   100: 9.606\n",
      "Loss after mini-batch   150: 6.801\n",
      "Loss after mini-batch   200: 5.218\n",
      "Loss after mini-batch    50: 3.656\n",
      "Loss after mini-batch   100: 3.535\n",
      "Loss after mini-batch   150: 3.214\n",
      "Loss after mini-batch   200: 3.208\n",
      "Loss after mini-batch    50: 3.100\n",
      "Loss after mini-batch   100: 3.079\n",
      "Loss after mini-batch   150: 3.138\n",
      "Loss after mini-batch   200: 2.957\n",
      "Loss after mini-batch    50: 2.978\n",
      "Loss after mini-batch   100: 2.843\n",
      "Loss after mini-batch   150: 2.925\n",
      "Loss after mini-batch   200: 3.078\n",
      "Loss after mini-batch    50: 3.053\n",
      "Loss after mini-batch   100: 2.861\n",
      "Loss after mini-batch   150: 3.265\n",
      "Loss after mini-batch   200: 3.212\n",
      "Loss after mini-batch    50: 2.937\n",
      "Loss after mini-batch   100: 2.844\n",
      "Loss after mini-batch   150: 2.904\n",
      "Loss after mini-batch   200: 2.943\n",
      "Loss after mini-batch    50: 2.996\n",
      "Loss after mini-batch   100: 2.972\n",
      "Loss after mini-batch   150: 3.049\n",
      "Loss after mini-batch   200: 2.873\n",
      "Loss after mini-batch    50: 2.994\n",
      "Loss after mini-batch   100: 2.980\n",
      "Loss after mini-batch   150: 2.825\n",
      "Loss after mini-batch   200: 2.792\n",
      "Loss after mini-batch    50: 2.865\n",
      "Loss after mini-batch   100: 2.859\n",
      "Loss after mini-batch   150: 2.904\n",
      "Loss after mini-batch   200: 2.731\n",
      "Loss after mini-batch    50: 3.096\n",
      "Loss after mini-batch   100: 3.009\n",
      "Loss after mini-batch   150: 2.783\n",
      "Loss after mini-batch   200: 2.992\n",
      "Loss after mini-batch    50: 2.799\n",
      "Loss after mini-batch   100: 3.059\n",
      "Loss after mini-batch   150: 2.895\n",
      "Loss after mini-batch   200: 2.701\n",
      "Loss after mini-batch    50: 2.966\n",
      "Loss after mini-batch   100: 2.695\n",
      "Loss after mini-batch   150: 3.031\n",
      "Loss after mini-batch   200: 2.690\n",
      "Loss after mini-batch    50: 2.980\n",
      "Loss after mini-batch   100: 2.851\n",
      "Loss after mini-batch   150: 2.902\n",
      "Loss after mini-batch   200: 2.932\n",
      "Loss after mini-batch    50: 2.896\n",
      "Loss after mini-batch   100: 2.868\n",
      "Loss after mini-batch   150: 2.947\n",
      "Loss after mini-batch   200: 3.010\n",
      "Starting epoch 2\n",
      "Loss after mini-batch    50: 2.779\n",
      "Loss after mini-batch   100: 2.727\n",
      "Loss after mini-batch   150: 2.777\n",
      "Loss after mini-batch   200: 2.849\n",
      "Loss after mini-batch    50: 2.952\n",
      "Loss after mini-batch   100: 2.692\n",
      "Loss after mini-batch   150: 2.727\n",
      "Loss after mini-batch   200: 3.083\n",
      "Loss after mini-batch    50: 2.959\n",
      "Loss after mini-batch   100: 2.832\n",
      "Loss after mini-batch   150: 2.837\n",
      "Loss after mini-batch   200: 2.776\n",
      "Loss after mini-batch    50: 2.893\n",
      "Loss after mini-batch   100: 2.841\n",
      "Loss after mini-batch   150: 2.794\n",
      "Loss after mini-batch   200: 2.767\n",
      "Loss after mini-batch    50: 2.914\n",
      "Loss after mini-batch   100: 2.813\n",
      "Loss after mini-batch   150: 2.671\n",
      "Loss after mini-batch   200: 2.854\n",
      "Loss after mini-batch    50: 2.815\n",
      "Loss after mini-batch   100: 2.945\n",
      "Loss after mini-batch   150: 2.867\n",
      "Loss after mini-batch   200: 2.905\n",
      "Loss after mini-batch    50: 2.740\n",
      "Loss after mini-batch   100: 3.001\n",
      "Loss after mini-batch   150: 2.820\n",
      "Loss after mini-batch   200: 2.868\n",
      "Loss after mini-batch    50: 2.952\n",
      "Loss after mini-batch   100: 2.837\n",
      "Loss after mini-batch   150: 2.854\n",
      "Loss after mini-batch   200: 2.925\n",
      "Loss after mini-batch    50: 2.924\n",
      "Loss after mini-batch   100: 2.672\n",
      "Loss after mini-batch   150: 2.685\n",
      "Loss after mini-batch   200: 2.717\n",
      "Loss after mini-batch    50: 2.820\n",
      "Loss after mini-batch   100: 2.680\n",
      "Loss after mini-batch   150: 2.775\n",
      "Loss after mini-batch   200: 2.811\n",
      "Loss after mini-batch    50: 3.056\n",
      "Loss after mini-batch   100: 2.715\n",
      "Loss after mini-batch   150: 2.905\n",
      "Loss after mini-batch   200: 2.998\n",
      "Loss after mini-batch    50: 2.958\n",
      "Loss after mini-batch   100: 2.815\n",
      "Loss after mini-batch   150: 2.559\n",
      "Loss after mini-batch   200: 2.850\n",
      "Loss after mini-batch    50: 2.684\n",
      "Loss after mini-batch   100: 2.695\n",
      "Loss after mini-batch   150: 2.872\n",
      "Loss after mini-batch   200: 2.794\n",
      "Loss after mini-batch    50: 2.828\n",
      "Loss after mini-batch   100: 2.809\n",
      "Loss after mini-batch   150: 2.884\n",
      "Loss after mini-batch   200: 2.837\n",
      "Loss after mini-batch    50: 2.911\n",
      "Loss after mini-batch   100: 2.889\n",
      "Loss after mini-batch   150: 2.864\n",
      "Loss after mini-batch   200: 2.883\n",
      "Starting epoch 3\n",
      "Loss after mini-batch    50: 2.716\n",
      "Loss after mini-batch   100: 2.864\n",
      "Loss after mini-batch   150: 2.770\n",
      "Loss after mini-batch   200: 2.813\n",
      "Loss after mini-batch    50: 2.898\n",
      "Loss after mini-batch   100: 3.000\n",
      "Loss after mini-batch   150: 2.713\n",
      "Loss after mini-batch   200: 2.876\n",
      "Loss after mini-batch    50: 2.868\n",
      "Loss after mini-batch   100: 2.730\n",
      "Loss after mini-batch   150: 2.976\n",
      "Loss after mini-batch   200: 2.750\n",
      "Loss after mini-batch    50: 2.740\n",
      "Loss after mini-batch   100: 2.962\n",
      "Loss after mini-batch   150: 2.673\n",
      "Loss after mini-batch   200: 2.837\n",
      "Loss after mini-batch    50: 2.843\n",
      "Loss after mini-batch   100: 2.839\n",
      "Loss after mini-batch   150: 2.762\n",
      "Loss after mini-batch   200: 2.785\n",
      "Loss after mini-batch    50: 2.813\n",
      "Loss after mini-batch   100: 2.883\n",
      "Loss after mini-batch   150: 2.754\n",
      "Loss after mini-batch   200: 2.919\n",
      "Loss after mini-batch    50: 2.865\n",
      "Loss after mini-batch   100: 2.645\n",
      "Loss after mini-batch   150: 2.863\n",
      "Loss after mini-batch   200: 2.874\n",
      "Loss after mini-batch    50: 2.762\n",
      "Loss after mini-batch   100: 2.955\n",
      "Loss after mini-batch   150: 2.930\n",
      "Loss after mini-batch   200: 2.860\n",
      "Loss after mini-batch    50: 2.879\n",
      "Loss after mini-batch   100: 2.784\n",
      "Loss after mini-batch   150: 2.860\n",
      "Loss after mini-batch   200: 2.709\n",
      "Loss after mini-batch    50: 2.655\n",
      "Loss after mini-batch   100: 2.835\n",
      "Loss after mini-batch   150: 2.678\n",
      "Loss after mini-batch   200: 2.906\n",
      "Loss after mini-batch    50: 2.763\n",
      "Loss after mini-batch   100: 2.668\n",
      "Loss after mini-batch   150: 3.147\n",
      "Loss after mini-batch   200: 2.887\n",
      "Loss after mini-batch    50: 2.687\n",
      "Loss after mini-batch   100: 2.927\n",
      "Loss after mini-batch   150: 2.831\n",
      "Loss after mini-batch   200: 2.768\n",
      "Loss after mini-batch    50: 2.811\n",
      "Loss after mini-batch   100: 2.760\n",
      "Loss after mini-batch   150: 2.763\n",
      "Loss after mini-batch   200: 2.726\n",
      "Loss after mini-batch    50: 2.771\n",
      "Loss after mini-batch   100: 2.938\n",
      "Loss after mini-batch   150: 2.838\n",
      "Loss after mini-batch   200: 2.698\n",
      "Loss after mini-batch    50: 2.775\n",
      "Loss after mini-batch   100: 2.961\n",
      "Loss after mini-batch   150: 2.905\n",
      "Loss after mini-batch   200: 2.833\n",
      "Starting epoch 4\n",
      "Loss after mini-batch    50: 2.843\n",
      "Loss after mini-batch   100: 2.745\n",
      "Loss after mini-batch   150: 2.696\n",
      "Loss after mini-batch   200: 2.868\n",
      "Loss after mini-batch    50: 2.866\n",
      "Loss after mini-batch   100: 2.786\n",
      "Loss after mini-batch   150: 2.724\n",
      "Loss after mini-batch   200: 2.797\n",
      "Loss after mini-batch    50: 2.699\n",
      "Loss after mini-batch   100: 2.847\n",
      "Loss after mini-batch   150: 2.768\n",
      "Loss after mini-batch   200: 2.721\n",
      "Loss after mini-batch    50: 2.763\n",
      "Loss after mini-batch   100: 2.763\n",
      "Loss after mini-batch   150: 2.807\n",
      "Loss after mini-batch   200: 2.685\n",
      "Loss after mini-batch    50: 2.756\n",
      "Loss after mini-batch   100: 2.786\n",
      "Loss after mini-batch   150: 2.776\n",
      "Loss after mini-batch   200: 2.819\n",
      "Loss after mini-batch    50: 3.038\n",
      "Loss after mini-batch   100: 2.691\n",
      "Loss after mini-batch   150: 2.809\n",
      "Loss after mini-batch   200: 2.760\n",
      "Loss after mini-batch    50: 2.781\n",
      "Loss after mini-batch   100: 2.833\n",
      "Loss after mini-batch   150: 2.751\n",
      "Loss after mini-batch   200: 2.940\n",
      "Loss after mini-batch    50: 2.919\n",
      "Loss after mini-batch   100: 2.706\n",
      "Loss after mini-batch   150: 3.036\n",
      "Loss after mini-batch   200: 2.675\n",
      "Loss after mini-batch    50: 2.886\n",
      "Loss after mini-batch   100: 2.833\n",
      "Loss after mini-batch   150: 2.745\n",
      "Loss after mini-batch   200: 2.690\n",
      "Loss after mini-batch    50: 2.769\n",
      "Loss after mini-batch   100: 2.594\n",
      "Loss after mini-batch   150: 2.801\n",
      "Loss after mini-batch   200: 2.817\n",
      "Loss after mini-batch    50: 3.094\n",
      "Loss after mini-batch   100: 2.667\n",
      "Loss after mini-batch   150: 2.776\n",
      "Loss after mini-batch   200: 2.774\n",
      "Loss after mini-batch    50: 2.708\n",
      "Loss after mini-batch   100: 2.892\n",
      "Loss after mini-batch   150: 2.800\n",
      "Loss after mini-batch   200: 2.690\n",
      "Loss after mini-batch    50: 2.858\n",
      "Loss after mini-batch   100: 2.684\n",
      "Loss after mini-batch   150: 2.727\n",
      "Loss after mini-batch   200: 2.762\n",
      "Loss after mini-batch    50: 2.975\n",
      "Loss after mini-batch   100: 2.839\n",
      "Loss after mini-batch   150: 2.735\n",
      "Loss after mini-batch   200: 2.799\n",
      "Loss after mini-batch    50: 3.147\n",
      "Loss after mini-batch   100: 2.877\n",
      "Loss after mini-batch   150: 2.809\n",
      "Loss after mini-batch   200: 2.709\n",
      "Starting epoch 5\n",
      "Loss after mini-batch    50: 2.809\n",
      "Loss after mini-batch   100: 2.716\n",
      "Loss after mini-batch   150: 2.687\n",
      "Loss after mini-batch   200: 2.763\n",
      "Loss after mini-batch    50: 2.733\n",
      "Loss after mini-batch   100: 2.773\n",
      "Loss after mini-batch   150: 2.821\n",
      "Loss after mini-batch   200: 2.807\n",
      "Loss after mini-batch    50: 2.790\n",
      "Loss after mini-batch   100: 2.787\n",
      "Loss after mini-batch   150: 2.720\n",
      "Loss after mini-batch   200: 2.763\n",
      "Loss after mini-batch    50: 2.731\n",
      "Loss after mini-batch   100: 2.809\n",
      "Loss after mini-batch   150: 2.673\n",
      "Loss after mini-batch   200: 2.825\n",
      "Loss after mini-batch    50: 2.708\n",
      "Loss after mini-batch   100: 2.815\n",
      "Loss after mini-batch   150: 2.867\n",
      "Loss after mini-batch   200: 2.689\n",
      "Loss after mini-batch    50: 3.003\n",
      "Loss after mini-batch   100: 2.687\n",
      "Loss after mini-batch   150: 2.892\n",
      "Loss after mini-batch   200: 2.755\n",
      "Loss after mini-batch    50: 2.761\n",
      "Loss after mini-batch   100: 2.775\n",
      "Loss after mini-batch   150: 2.794\n",
      "Loss after mini-batch   200: 2.940\n",
      "Loss after mini-batch    50: 2.893\n",
      "Loss after mini-batch   100: 2.813\n",
      "Loss after mini-batch   150: 2.925\n",
      "Loss after mini-batch   200: 2.822\n",
      "Loss after mini-batch    50: 2.750\n",
      "Loss after mini-batch   100: 2.805\n",
      "Loss after mini-batch   150: 2.675\n",
      "Loss after mini-batch   200: 2.719\n",
      "Loss after mini-batch    50: 2.763\n",
      "Loss after mini-batch   100: 2.838\n",
      "Loss after mini-batch   150: 2.641\n",
      "Loss after mini-batch   200: 2.698\n",
      "Loss after mini-batch    50: 2.795\n",
      "Loss after mini-batch   100: 2.815\n",
      "Loss after mini-batch   150: 2.972\n",
      "Loss after mini-batch   200: 2.731\n",
      "Loss after mini-batch    50: 2.708\n",
      "Loss after mini-batch   100: 2.756\n",
      "Loss after mini-batch   150: 2.558\n",
      "Loss after mini-batch   200: 2.782\n",
      "Loss after mini-batch    50: 2.674\n",
      "Loss after mini-batch   100: 2.788\n",
      "Loss after mini-batch   150: 2.807\n",
      "Loss after mini-batch   200: 2.787\n",
      "Loss after mini-batch    50: 2.654\n",
      "Loss after mini-batch   100: 2.871\n",
      "Loss after mini-batch   150: 2.780\n",
      "Loss after mini-batch   200: 3.016\n",
      "Loss after mini-batch    50: 2.844\n",
      "Loss after mini-batch   100: 2.817\n",
      "Loss after mini-batch   150: 2.952\n",
      "Loss after mini-batch   200: 2.836\n",
      "Starting epoch 6\n",
      "Loss after mini-batch    50: 2.791\n",
      "Loss after mini-batch   100: 2.678\n",
      "Loss after mini-batch   150: 2.691\n",
      "Loss after mini-batch   200: 2.772\n",
      "Loss after mini-batch    50: 2.884\n",
      "Loss after mini-batch   100: 2.857\n",
      "Loss after mini-batch   150: 2.741\n",
      "Loss after mini-batch   200: 2.788\n",
      "Loss after mini-batch    50: 2.833\n",
      "Loss after mini-batch   100: 2.679\n",
      "Loss after mini-batch   150: 2.792\n",
      "Loss after mini-batch   200: 2.791\n",
      "Loss after mini-batch    50: 2.765\n",
      "Loss after mini-batch   100: 2.752\n",
      "Loss after mini-batch   150: 2.809\n",
      "Loss after mini-batch   200: 2.718\n",
      "Loss after mini-batch    50: 2.814\n",
      "Loss after mini-batch   100: 2.826\n",
      "Loss after mini-batch   150: 2.854\n",
      "Loss after mini-batch   200: 2.649\n",
      "Loss after mini-batch    50: 2.762\n",
      "Loss after mini-batch   100: 2.718\n",
      "Loss after mini-batch   150: 2.729\n",
      "Loss after mini-batch   200: 2.870\n",
      "Loss after mini-batch    50: 2.771\n",
      "Loss after mini-batch   100: 2.767\n",
      "Loss after mini-batch   150: 2.782\n",
      "Loss after mini-batch   200: 2.853\n",
      "Loss after mini-batch    50: 2.712\n",
      "Loss after mini-batch   100: 2.967\n",
      "Loss after mini-batch   150: 2.745\n",
      "Loss after mini-batch   200: 2.901\n",
      "Loss after mini-batch    50: 2.751\n",
      "Loss after mini-batch   100: 2.843\n",
      "Loss after mini-batch   150: 2.737\n",
      "Loss after mini-batch   200: 2.678\n",
      "Loss after mini-batch    50: 2.848\n",
      "Loss after mini-batch   100: 2.582\n",
      "Loss after mini-batch   150: 2.742\n",
      "Loss after mini-batch   200: 2.729\n",
      "Loss after mini-batch    50: 2.794\n",
      "Loss after mini-batch   100: 2.848\n",
      "Loss after mini-batch   150: 2.878\n",
      "Loss after mini-batch   200: 2.719\n",
      "Loss after mini-batch    50: 2.573\n",
      "Loss after mini-batch   100: 2.760\n",
      "Loss after mini-batch   150: 2.909\n",
      "Loss after mini-batch   200: 2.656\n",
      "Loss after mini-batch    50: 2.743\n",
      "Loss after mini-batch   100: 2.808\n",
      "Loss after mini-batch   150: 2.738\n",
      "Loss after mini-batch   200: 2.754\n",
      "Loss after mini-batch    50: 2.863\n",
      "Loss after mini-batch   100: 2.672\n",
      "Loss after mini-batch   150: 2.844\n",
      "Loss after mini-batch   200: 2.805\n",
      "Loss after mini-batch    50: 3.015\n",
      "Loss after mini-batch   100: 2.738\n",
      "Loss after mini-batch   150: 2.896\n",
      "Loss after mini-batch   200: 2.860\n",
      "Starting epoch 7\n",
      "Loss after mini-batch    50: 2.634\n",
      "Loss after mini-batch   100: 2.743\n",
      "Loss after mini-batch   150: 2.789\n",
      "Loss after mini-batch   200: 2.780\n",
      "Loss after mini-batch    50: 2.811\n",
      "Loss after mini-batch   100: 2.805\n",
      "Loss after mini-batch   150: 2.726\n",
      "Loss after mini-batch   200: 2.797\n",
      "Loss after mini-batch    50: 2.779\n",
      "Loss after mini-batch   100: 2.720\n",
      "Loss after mini-batch   150: 2.872\n",
      "Loss after mini-batch   200: 2.689\n",
      "Loss after mini-batch    50: 2.711\n",
      "Loss after mini-batch   100: 2.747\n",
      "Loss after mini-batch   150: 2.647\n",
      "Loss after mini-batch   200: 2.846\n",
      "Loss after mini-batch    50: 2.787\n",
      "Loss after mini-batch   100: 2.745\n",
      "Loss after mini-batch   150: 2.857\n",
      "Loss after mini-batch   200: 2.642\n",
      "Loss after mini-batch    50: 2.864\n",
      "Loss after mini-batch   100: 2.872\n",
      "Loss after mini-batch   150: 2.703\n",
      "Loss after mini-batch   200: 2.582\n",
      "Loss after mini-batch    50: 2.766\n",
      "Loss after mini-batch   100: 2.791\n",
      "Loss after mini-batch   150: 2.860\n",
      "Loss after mini-batch   200: 2.739\n",
      "Loss after mini-batch    50: 2.859\n",
      "Loss after mini-batch   100: 2.729\n",
      "Loss after mini-batch   150: 2.878\n",
      "Loss after mini-batch   200: 2.847\n",
      "Loss after mini-batch    50: 2.585\n",
      "Loss after mini-batch   100: 3.108\n",
      "Loss after mini-batch   150: 2.775\n",
      "Loss after mini-batch   200: 2.609\n",
      "Loss after mini-batch    50: 2.778\n",
      "Loss after mini-batch   100: 2.725\n",
      "Loss after mini-batch   150: 2.806\n",
      "Loss after mini-batch   200: 2.567\n",
      "Loss after mini-batch    50: 3.003\n",
      "Loss after mini-batch   100: 2.728\n",
      "Loss after mini-batch   150: 2.751\n",
      "Loss after mini-batch   200: 2.739\n",
      "Loss after mini-batch    50: 2.806\n",
      "Loss after mini-batch   100: 2.739\n",
      "Loss after mini-batch   150: 2.704\n",
      "Loss after mini-batch   200: 2.739\n",
      "Loss after mini-batch    50: 2.820\n",
      "Loss after mini-batch   100: 2.814\n",
      "Loss after mini-batch   150: 2.598\n",
      "Loss after mini-batch   200: 2.676\n",
      "Loss after mini-batch    50: 2.784\n",
      "Loss after mini-batch   100: 2.713\n",
      "Loss after mini-batch   150: 2.878\n",
      "Loss after mini-batch   200: 2.798\n",
      "Loss after mini-batch    50: 2.797\n",
      "Loss after mini-batch   100: 3.087\n",
      "Loss after mini-batch   150: 2.876\n",
      "Loss after mini-batch   200: 2.706\n",
      "Starting epoch 8\n",
      "Loss after mini-batch    50: 2.739\n",
      "Loss after mini-batch   100: 2.722\n",
      "Loss after mini-batch   150: 2.709\n",
      "Loss after mini-batch   200: 2.837\n",
      "Loss after mini-batch    50: 2.799\n",
      "Loss after mini-batch   100: 2.810\n",
      "Loss after mini-batch   150: 2.833\n",
      "Loss after mini-batch   200: 2.744\n",
      "Loss after mini-batch    50: 2.835\n",
      "Loss after mini-batch   100: 2.890\n",
      "Loss after mini-batch   150: 2.679\n",
      "Loss after mini-batch   200: 2.682\n",
      "Loss after mini-batch    50: 2.789\n",
      "Loss after mini-batch   100: 2.745\n",
      "Loss after mini-batch   150: 2.750\n",
      "Loss after mini-batch   200: 2.847\n",
      "Loss after mini-batch    50: 2.818\n",
      "Loss after mini-batch   100: 2.650\n",
      "Loss after mini-batch   150: 2.817\n",
      "Loss after mini-batch   200: 2.709\n",
      "Loss after mini-batch    50: 2.728\n",
      "Loss after mini-batch   100: 2.660\n",
      "Loss after mini-batch   150: 2.786\n",
      "Loss after mini-batch   200: 2.731\n",
      "Loss after mini-batch    50: 2.799\n",
      "Loss after mini-batch   100: 2.794\n",
      "Loss after mini-batch   150: 2.680\n",
      "Loss after mini-batch   200: 2.745\n",
      "Loss after mini-batch    50: 2.861\n",
      "Loss after mini-batch   100: 2.659\n",
      "Loss after mini-batch   150: 2.882\n",
      "Loss after mini-batch   200: 2.751\n",
      "Loss after mini-batch    50: 2.667\n",
      "Loss after mini-batch   100: 2.709\n",
      "Loss after mini-batch   150: 2.760\n",
      "Loss after mini-batch   200: 2.766\n",
      "Loss after mini-batch    50: 2.769\n",
      "Loss after mini-batch   100: 2.716\n",
      "Loss after mini-batch   150: 2.688\n",
      "Loss after mini-batch   200: 2.713\n",
      "Loss after mini-batch    50: 2.911\n",
      "Loss after mini-batch   100: 2.981\n",
      "Loss after mini-batch   150: 2.670\n",
      "Loss after mini-batch   200: 2.802\n",
      "Loss after mini-batch    50: 2.697\n",
      "Loss after mini-batch   100: 2.647\n",
      "Loss after mini-batch   150: 2.718\n",
      "Loss after mini-batch   200: 2.840\n",
      "Loss after mini-batch    50: 2.855\n",
      "Loss after mini-batch   100: 2.682\n",
      "Loss after mini-batch   150: 2.776\n",
      "Loss after mini-batch   200: 2.694\n",
      "Loss after mini-batch    50: 2.867\n",
      "Loss after mini-batch   100: 2.762\n",
      "Loss after mini-batch   150: 2.700\n",
      "Loss after mini-batch   200: 2.858\n",
      "Loss after mini-batch    50: 3.060\n",
      "Loss after mini-batch   100: 2.656\n",
      "Loss after mini-batch   150: 3.017\n",
      "Loss after mini-batch   200: 2.812\n",
      "Starting epoch 9\n",
      "Loss after mini-batch    50: 2.709\n",
      "Loss after mini-batch   100: 2.761\n",
      "Loss after mini-batch   150: 2.698\n",
      "Loss after mini-batch   200: 2.697\n",
      "Loss after mini-batch    50: 2.740\n",
      "Loss after mini-batch   100: 2.701\n",
      "Loss after mini-batch   150: 2.774\n",
      "Loss after mini-batch   200: 2.800\n",
      "Loss after mini-batch    50: 2.924\n",
      "Loss after mini-batch   100: 2.681\n",
      "Loss after mini-batch   150: 2.902\n",
      "Loss after mini-batch   200: 2.763\n",
      "Loss after mini-batch    50: 2.779\n",
      "Loss after mini-batch   100: 2.831\n",
      "Loss after mini-batch   150: 2.729\n",
      "Loss after mini-batch   200: 2.668\n",
      "Loss after mini-batch    50: 2.900\n",
      "Loss after mini-batch   100: 2.722\n",
      "Loss after mini-batch   150: 2.683\n",
      "Loss after mini-batch   200: 2.675\n",
      "Loss after mini-batch    50: 2.694\n",
      "Loss after mini-batch   100: 2.804\n",
      "Loss after mini-batch   150: 2.840\n",
      "Loss after mini-batch   200: 2.640\n",
      "Loss after mini-batch    50: 2.873\n",
      "Loss after mini-batch   100: 2.644\n",
      "Loss after mini-batch   150: 2.826\n",
      "Loss after mini-batch   200: 2.762\n",
      "Loss after mini-batch    50: 2.731\n",
      "Loss after mini-batch   100: 2.911\n",
      "Loss after mini-batch   150: 2.959\n",
      "Loss after mini-batch   200: 2.810\n",
      "Loss after mini-batch    50: 2.678\n",
      "Loss after mini-batch   100: 2.695\n",
      "Loss after mini-batch   150: 2.701\n",
      "Loss after mini-batch   200: 2.876\n",
      "Loss after mini-batch    50: 2.690\n",
      "Loss after mini-batch   100: 2.837\n",
      "Loss after mini-batch   150: 2.664\n",
      "Loss after mini-batch   200: 2.642\n",
      "Loss after mini-batch    50: 2.784\n",
      "Loss after mini-batch   100: 3.036\n",
      "Loss after mini-batch   150: 2.760\n",
      "Loss after mini-batch   200: 2.643\n",
      "Loss after mini-batch    50: 2.773\n",
      "Loss after mini-batch   100: 2.655\n",
      "Loss after mini-batch   150: 2.792\n",
      "Loss after mini-batch   200: 2.591\n",
      "Loss after mini-batch    50: 2.809\n",
      "Loss after mini-batch   100: 2.613\n",
      "Loss after mini-batch   150: 2.619\n",
      "Loss after mini-batch   200: 2.833\n",
      "Loss after mini-batch    50: 2.759\n",
      "Loss after mini-batch   100: 2.738\n",
      "Loss after mini-batch   150: 2.706\n",
      "Loss after mini-batch   200: 2.850\n",
      "Loss after mini-batch    50: 2.789\n",
      "Loss after mini-batch   100: 3.036\n",
      "Loss after mini-batch   150: 2.892\n",
      "Loss after mini-batch   200: 2.887\n",
      "Starting epoch 10\n",
      "Loss after mini-batch    50: 2.765\n",
      "Loss after mini-batch   100: 2.750\n",
      "Loss after mini-batch   150: 2.746\n",
      "Loss after mini-batch   200: 2.693\n",
      "Loss after mini-batch    50: 2.784\n",
      "Loss after mini-batch   100: 2.623\n",
      "Loss after mini-batch   150: 2.659\n",
      "Loss after mini-batch   200: 2.792\n",
      "Loss after mini-batch    50: 2.755\n",
      "Loss after mini-batch   100: 2.760\n",
      "Loss after mini-batch   150: 2.781\n",
      "Loss after mini-batch   200: 2.867\n",
      "Loss after mini-batch    50: 2.758\n",
      "Loss after mini-batch   100: 2.765\n",
      "Loss after mini-batch   150: 2.724\n",
      "Loss after mini-batch   200: 2.820\n",
      "Loss after mini-batch    50: 2.836\n",
      "Loss after mini-batch   100: 2.620\n",
      "Loss after mini-batch   150: 2.766\n",
      "Loss after mini-batch   200: 2.682\n",
      "Loss after mini-batch    50: 2.875\n",
      "Loss after mini-batch   100: 2.675\n",
      "Loss after mini-batch   150: 2.779\n",
      "Loss after mini-batch   200: 2.737\n",
      "Loss after mini-batch    50: 2.629\n",
      "Loss after mini-batch   100: 3.115\n",
      "Loss after mini-batch   150: 2.710\n",
      "Loss after mini-batch   200: 2.660\n",
      "Loss after mini-batch    50: 3.009\n",
      "Loss after mini-batch   100: 2.771\n",
      "Loss after mini-batch   150: 2.744\n",
      "Loss after mini-batch   200: 2.751\n",
      "Loss after mini-batch    50: 2.701\n",
      "Loss after mini-batch   100: 2.740\n",
      "Loss after mini-batch   150: 2.817\n",
      "Loss after mini-batch   200: 2.702\n",
      "Loss after mini-batch    50: 2.762\n",
      "Loss after mini-batch   100: 2.705\n",
      "Loss after mini-batch   150: 2.639\n",
      "Loss after mini-batch   200: 2.671\n",
      "Loss after mini-batch    50: 2.896\n",
      "Loss after mini-batch   100: 2.961\n",
      "Loss after mini-batch   150: 2.769\n",
      "Loss after mini-batch   200: 2.636\n",
      "Loss after mini-batch    50: 2.714\n",
      "Loss after mini-batch   100: 2.768\n",
      "Loss after mini-batch   150: 2.687\n",
      "Loss after mini-batch   200: 2.663\n",
      "Loss after mini-batch    50: 2.882\n",
      "Loss after mini-batch   100: 2.667\n",
      "Loss after mini-batch   150: 2.654\n",
      "Loss after mini-batch   200: 2.763\n",
      "Loss after mini-batch    50: 2.599\n",
      "Loss after mini-batch   100: 2.833\n",
      "Loss after mini-batch   150: 2.860\n",
      "Loss after mini-batch   200: 2.699\n",
      "Loss after mini-batch    50: 2.819\n",
      "Loss after mini-batch   100: 2.866\n",
      "Loss after mini-batch   150: 3.061\n",
      "Loss after mini-batch   200: 2.785\n",
      "Starting epoch 11\n",
      "Loss after mini-batch    50: 2.704\n",
      "Loss after mini-batch   100: 2.869\n",
      "Loss after mini-batch   150: 2.737\n",
      "Loss after mini-batch   200: 2.588\n",
      "Loss after mini-batch    50: 2.727\n",
      "Loss after mini-batch   100: 2.811\n",
      "Loss after mini-batch   150: 2.788\n",
      "Loss after mini-batch   200: 2.760\n",
      "Loss after mini-batch    50: 2.799\n",
      "Loss after mini-batch   100: 2.938\n",
      "Loss after mini-batch   150: 2.773\n",
      "Loss after mini-batch   200: 2.612\n",
      "Loss after mini-batch    50: 2.772\n",
      "Loss after mini-batch   100: 2.592\n",
      "Loss after mini-batch   150: 2.760\n",
      "Loss after mini-batch   200: 2.843\n",
      "Loss after mini-batch    50: 2.803\n",
      "Loss after mini-batch   100: 2.847\n",
      "Loss after mini-batch   150: 2.707\n",
      "Loss after mini-batch   200: 2.666\n",
      "Loss after mini-batch    50: 2.780\n",
      "Loss after mini-batch   100: 2.648\n",
      "Loss after mini-batch   150: 2.892\n",
      "Loss after mini-batch   200: 2.728\n",
      "Loss after mini-batch    50: 2.899\n",
      "Loss after mini-batch   100: 2.667\n",
      "Loss after mini-batch   150: 2.799\n",
      "Loss after mini-batch   200: 2.638\n",
      "Loss after mini-batch    50: 2.760\n",
      "Loss after mini-batch   100: 2.824\n",
      "Loss after mini-batch   150: 2.765\n",
      "Loss after mini-batch   200: 2.772\n",
      "Loss after mini-batch    50: 2.776\n",
      "Loss after mini-batch   100: 2.741\n",
      "Loss after mini-batch   150: 2.745\n",
      "Loss after mini-batch   200: 2.757\n",
      "Loss after mini-batch    50: 2.498\n",
      "Loss after mini-batch   100: 2.695\n",
      "Loss after mini-batch   150: 2.677\n",
      "Loss after mini-batch   200: 2.634\n",
      "Loss after mini-batch    50: 2.759\n",
      "Loss after mini-batch   100: 2.999\n",
      "Loss after mini-batch   150: 2.680\n",
      "Loss after mini-batch   200: 2.735\n",
      "Loss after mini-batch    50: 2.647\n",
      "Loss after mini-batch   100: 2.738\n",
      "Loss after mini-batch   150: 2.697\n",
      "Loss after mini-batch   200: 2.840\n",
      "Loss after mini-batch    50: 2.848\n",
      "Loss after mini-batch   100: 2.685\n",
      "Loss after mini-batch   150: 2.763\n",
      "Loss after mini-batch   200: 2.616\n",
      "Loss after mini-batch    50: 2.790\n",
      "Loss after mini-batch   100: 2.763\n",
      "Loss after mini-batch   150: 2.910\n",
      "Loss after mini-batch   200: 2.757\n",
      "Loss after mini-batch    50: 2.799\n",
      "Loss after mini-batch   100: 2.756\n",
      "Loss after mini-batch   150: 2.841\n",
      "Loss after mini-batch   200: 2.785\n",
      "Starting epoch 12\n",
      "Loss after mini-batch    50: 2.727\n",
      "Loss after mini-batch   100: 2.696\n",
      "Loss after mini-batch   150: 2.817\n",
      "Loss after mini-batch   200: 2.723\n",
      "Loss after mini-batch    50: 2.813\n",
      "Loss after mini-batch   100: 2.684\n",
      "Loss after mini-batch   150: 2.723\n",
      "Loss after mini-batch   200: 2.724\n",
      "Loss after mini-batch    50: 2.805\n",
      "Loss after mini-batch   100: 2.746\n",
      "Loss after mini-batch   150: 2.687\n",
      "Loss after mini-batch   200: 2.750\n",
      "Loss after mini-batch    50: 2.663\n",
      "Loss after mini-batch   100: 2.790\n",
      "Loss after mini-batch   150: 2.752\n",
      "Loss after mini-batch   200: 2.748\n",
      "Loss after mini-batch    50: 2.699\n",
      "Loss after mini-batch   100: 2.797\n",
      "Loss after mini-batch   150: 2.673\n",
      "Loss after mini-batch   200: 2.865\n",
      "Loss after mini-batch    50: 2.708\n",
      "Loss after mini-batch   100: 2.651\n",
      "Loss after mini-batch   150: 2.917\n",
      "Loss after mini-batch   200: 2.791\n",
      "Loss after mini-batch    50: 2.770\n",
      "Loss after mini-batch   100: 2.776\n",
      "Loss after mini-batch   150: 2.826\n",
      "Loss after mini-batch   200: 2.859\n",
      "Loss after mini-batch    50: 2.876\n",
      "Loss after mini-batch   100: 2.737\n",
      "Loss after mini-batch   150: 2.842\n",
      "Loss after mini-batch   200: 2.815\n",
      "Loss after mini-batch    50: 2.668\n",
      "Loss after mini-batch   100: 2.805\n",
      "Loss after mini-batch   150: 2.795\n",
      "Loss after mini-batch   200: 2.740\n",
      "Loss after mini-batch    50: 2.654\n",
      "Loss after mini-batch   100: 2.626\n",
      "Loss after mini-batch   150: 2.853\n",
      "Loss after mini-batch   200: 2.684\n",
      "Loss after mini-batch    50: 2.796\n",
      "Loss after mini-batch   100: 2.669\n",
      "Loss after mini-batch   150: 2.663\n",
      "Loss after mini-batch   200: 2.788\n",
      "Loss after mini-batch    50: 2.811\n",
      "Loss after mini-batch   100: 2.650\n",
      "Loss after mini-batch   150: 2.782\n",
      "Loss after mini-batch   200: 2.719\n",
      "Loss after mini-batch    50: 2.749\n",
      "Loss after mini-batch   100: 2.752\n",
      "Loss after mini-batch   150: 2.656\n",
      "Loss after mini-batch   200: 2.798\n",
      "Loss after mini-batch    50: 2.816\n",
      "Loss after mini-batch   100: 3.044\n",
      "Loss after mini-batch   150: 2.612\n",
      "Loss after mini-batch   200: 2.553\n",
      "Loss after mini-batch    50: 2.880\n",
      "Loss after mini-batch   100: 2.748\n",
      "Loss after mini-batch   150: 2.820\n",
      "Loss after mini-batch   200: 2.925\n",
      "Starting epoch 13\n",
      "Loss after mini-batch    50: 2.691\n",
      "Loss after mini-batch   100: 2.725\n",
      "Loss after mini-batch   150: 2.710\n",
      "Loss after mini-batch   200: 2.633\n",
      "Loss after mini-batch    50: 2.699\n",
      "Loss after mini-batch   100: 2.746\n",
      "Loss after mini-batch   150: 2.794\n",
      "Loss after mini-batch   200: 2.871\n",
      "Loss after mini-batch    50: 2.753\n",
      "Loss after mini-batch   100: 2.713\n",
      "Loss after mini-batch   150: 2.940\n",
      "Loss after mini-batch   200: 2.762\n",
      "Loss after mini-batch    50: 2.753\n",
      "Loss after mini-batch   100: 2.828\n",
      "Loss after mini-batch   150: 2.800\n",
      "Loss after mini-batch   200: 2.679\n",
      "Loss after mini-batch    50: 2.733\n",
      "Loss after mini-batch   100: 2.582\n",
      "Loss after mini-batch   150: 2.832\n",
      "Loss after mini-batch   200: 2.823\n",
      "Loss after mini-batch    50: 2.840\n",
      "Loss after mini-batch   100: 2.742\n",
      "Loss after mini-batch   150: 2.649\n",
      "Loss after mini-batch   200: 2.809\n",
      "Loss after mini-batch    50: 2.911\n",
      "Loss after mini-batch   100: 2.633\n",
      "Loss after mini-batch   150: 2.711\n",
      "Loss after mini-batch   200: 2.775\n",
      "Loss after mini-batch    50: 2.885\n",
      "Loss after mini-batch   100: 2.854\n",
      "Loss after mini-batch   150: 2.764\n",
      "Loss after mini-batch   200: 2.843\n",
      "Loss after mini-batch    50: 2.734\n",
      "Loss after mini-batch   100: 2.746\n",
      "Loss after mini-batch   150: 2.868\n",
      "Loss after mini-batch   200: 2.604\n",
      "Loss after mini-batch    50: 2.752\n",
      "Loss after mini-batch   100: 2.656\n",
      "Loss after mini-batch   150: 2.695\n",
      "Loss after mini-batch   200: 2.664\n",
      "Loss after mini-batch    50: 2.677\n",
      "Loss after mini-batch   100: 2.828\n",
      "Loss after mini-batch   150: 2.789\n",
      "Loss after mini-batch   200: 2.653\n",
      "Loss after mini-batch    50: 2.631\n",
      "Loss after mini-batch   100: 2.701\n",
      "Loss after mini-batch   150: 2.681\n",
      "Loss after mini-batch   200: 2.839\n",
      "Loss after mini-batch    50: 2.598\n",
      "Loss after mini-batch   100: 2.894\n",
      "Loss after mini-batch   150: 2.646\n",
      "Loss after mini-batch   200: 2.710\n",
      "Loss after mini-batch    50: 2.804\n",
      "Loss after mini-batch   100: 2.728\n",
      "Loss after mini-batch   150: 2.860\n",
      "Loss after mini-batch   200: 2.750\n",
      "Loss after mini-batch    50: 2.846\n",
      "Loss after mini-batch   100: 2.985\n",
      "Loss after mini-batch   150: 2.927\n",
      "Loss after mini-batch   200: 2.819\n",
      "Starting epoch 14\n",
      "Loss after mini-batch    50: 2.776\n",
      "Loss after mini-batch   100: 2.813\n",
      "Loss after mini-batch   150: 2.640\n",
      "Loss after mini-batch   200: 2.656\n",
      "Loss after mini-batch    50: 2.815\n",
      "Loss after mini-batch   100: 2.624\n",
      "Loss after mini-batch   150: 2.720\n",
      "Loss after mini-batch   200: 2.886\n",
      "Loss after mini-batch    50: 2.713\n",
      "Loss after mini-batch   100: 2.698\n",
      "Loss after mini-batch   150: 2.681\n",
      "Loss after mini-batch   200: 2.784\n",
      "Loss after mini-batch    50: 2.772\n",
      "Loss after mini-batch   100: 2.802\n",
      "Loss after mini-batch   150: 2.704\n",
      "Loss after mini-batch   200: 2.605\n",
      "Loss after mini-batch    50: 2.930\n",
      "Loss after mini-batch   100: 2.642\n",
      "Loss after mini-batch   150: 2.743\n",
      "Loss after mini-batch   200: 2.637\n",
      "Loss after mini-batch    50: 2.775\n",
      "Loss after mini-batch   100: 2.715\n",
      "Loss after mini-batch   150: 2.763\n",
      "Loss after mini-batch   200: 2.756\n",
      "Loss after mini-batch    50: 2.761\n",
      "Loss after mini-batch   100: 2.751\n",
      "Loss after mini-batch   150: 2.967\n",
      "Loss after mini-batch   200: 2.623\n",
      "Loss after mini-batch    50: 2.808\n",
      "Loss after mini-batch   100: 2.774\n",
      "Loss after mini-batch   150: 2.798\n",
      "Loss after mini-batch   200: 2.906\n",
      "Loss after mini-batch    50: 2.929\n",
      "Loss after mini-batch   100: 2.768\n",
      "Loss after mini-batch   150: 2.585\n",
      "Loss after mini-batch   200: 2.600\n",
      "Loss after mini-batch    50: 2.682\n",
      "Loss after mini-batch   100: 2.665\n",
      "Loss after mini-batch   150: 2.784\n",
      "Loss after mini-batch   200: 2.821\n",
      "Loss after mini-batch    50: 2.800\n",
      "Loss after mini-batch   100: 2.879\n",
      "Loss after mini-batch   150: 2.672\n",
      "Loss after mini-batch   200: 2.903\n",
      "Loss after mini-batch    50: 2.632\n",
      "Loss after mini-batch   100: 2.754\n",
      "Loss after mini-batch   150: 2.778\n",
      "Loss after mini-batch   200: 2.671\n",
      "Loss after mini-batch    50: 2.735\n",
      "Loss after mini-batch   100: 2.740\n",
      "Loss after mini-batch   150: 2.587\n",
      "Loss after mini-batch   200: 2.720\n",
      "Loss after mini-batch    50: 2.768\n",
      "Loss after mini-batch   100: 2.815\n",
      "Loss after mini-batch   150: 2.734\n",
      "Loss after mini-batch   200: 2.801\n",
      "Loss after mini-batch    50: 2.905\n",
      "Loss after mini-batch   100: 2.857\n",
      "Loss after mini-batch   150: 2.758\n",
      "Loss after mini-batch   200: 2.876\n",
      "Starting epoch 15\n",
      "Loss after mini-batch    50: 2.758\n",
      "Loss after mini-batch   100: 2.594\n",
      "Loss after mini-batch   150: 2.938\n",
      "Loss after mini-batch   200: 2.706\n",
      "Loss after mini-batch    50: 3.006\n",
      "Loss after mini-batch   100: 2.631\n",
      "Loss after mini-batch   150: 2.709\n",
      "Loss after mini-batch   200: 2.708\n",
      "Loss after mini-batch    50: 2.716\n",
      "Loss after mini-batch   100: 2.758\n",
      "Loss after mini-batch   150: 2.705\n",
      "Loss after mini-batch   200: 2.858\n",
      "Loss after mini-batch    50: 2.620\n",
      "Loss after mini-batch   100: 2.798\n",
      "Loss after mini-batch   150: 2.871\n",
      "Loss after mini-batch   200: 2.722\n",
      "Loss after mini-batch    50: 2.799\n",
      "Loss after mini-batch   100: 2.563\n",
      "Loss after mini-batch   150: 2.808\n",
      "Loss after mini-batch   200: 2.744\n",
      "Loss after mini-batch    50: 2.882\n",
      "Loss after mini-batch   100: 2.715\n",
      "Loss after mini-batch   150: 2.699\n",
      "Loss after mini-batch   200: 2.641\n",
      "Loss after mini-batch    50: 2.728\n",
      "Loss after mini-batch   100: 2.706\n",
      "Loss after mini-batch   150: 2.882\n",
      "Loss after mini-batch   200: 2.666\n",
      "Loss after mini-batch    50: 3.021\n",
      "Loss after mini-batch   100: 2.791\n",
      "Loss after mini-batch   150: 2.612\n",
      "Loss after mini-batch   200: 2.692\n",
      "Loss after mini-batch    50: 2.745\n",
      "Loss after mini-batch   100: 2.693\n",
      "Loss after mini-batch   150: 2.685\n",
      "Loss after mini-batch   200: 2.821\n",
      "Loss after mini-batch    50: 2.863\n",
      "Loss after mini-batch   100: 2.637\n",
      "Loss after mini-batch   150: 2.569\n",
      "Loss after mini-batch   200: 2.733\n",
      "Loss after mini-batch    50: 2.735\n",
      "Loss after mini-batch   100: 2.849\n",
      "Loss after mini-batch   150: 2.718\n",
      "Loss after mini-batch   200: 2.695\n",
      "Loss after mini-batch    50: 2.709\n",
      "Loss after mini-batch   100: 2.759\n",
      "Loss after mini-batch   150: 2.627\n",
      "Loss after mini-batch   200: 2.724\n",
      "Loss after mini-batch    50: 2.699\n",
      "Loss after mini-batch   100: 2.746\n",
      "Loss after mini-batch   150: 2.728\n",
      "Loss after mini-batch   200: 2.672\n",
      "Loss after mini-batch    50: 2.642\n",
      "Loss after mini-batch   100: 2.710\n",
      "Loss after mini-batch   150: 2.813\n",
      "Loss after mini-batch   200: 2.727\n",
      "Loss after mini-batch    50: 2.749\n",
      "Loss after mini-batch   100: 3.016\n",
      "Loss after mini-batch   150: 2.853\n",
      "Loss after mini-batch   200: 2.821\n",
      "Starting epoch 16\n",
      "Loss after mini-batch    50: 2.682\n",
      "Loss after mini-batch   100: 2.737\n",
      "Loss after mini-batch   150: 2.604\n",
      "Loss after mini-batch   200: 2.683\n",
      "Loss after mini-batch    50: 2.782\n",
      "Loss after mini-batch   100: 2.754\n",
      "Loss after mini-batch   150: 2.738\n",
      "Loss after mini-batch   200: 2.726\n",
      "Loss after mini-batch    50: 2.800\n",
      "Loss after mini-batch   100: 2.703\n",
      "Loss after mini-batch   150: 2.790\n",
      "Loss after mini-batch   200: 2.707\n",
      "Loss after mini-batch    50: 2.850\n",
      "Loss after mini-batch   100: 2.649\n",
      "Loss after mini-batch   150: 2.808\n",
      "Loss after mini-batch   200: 2.690\n",
      "Loss after mini-batch    50: 2.722\n",
      "Loss after mini-batch   100: 2.751\n",
      "Loss after mini-batch   150: 2.665\n",
      "Loss after mini-batch   200: 2.764\n",
      "Loss after mini-batch    50: 2.706\n",
      "Loss after mini-batch   100: 2.708\n",
      "Loss after mini-batch   150: 2.713\n",
      "Loss after mini-batch   200: 2.757\n",
      "Loss after mini-batch    50: 2.877\n",
      "Loss after mini-batch   100: 2.619\n",
      "Loss after mini-batch   150: 2.672\n",
      "Loss after mini-batch   200: 2.940\n",
      "Loss after mini-batch    50: 2.865\n",
      "Loss after mini-batch   100: 2.923\n",
      "Loss after mini-batch   150: 2.759\n",
      "Loss after mini-batch   200: 2.640\n",
      "Loss after mini-batch    50: 2.790\n",
      "Loss after mini-batch   100: 2.817\n",
      "Loss after mini-batch   150: 2.753\n",
      "Loss after mini-batch   200: 2.664\n",
      "Loss after mini-batch    50: 2.803\n",
      "Loss after mini-batch   100: 2.695\n",
      "Loss after mini-batch   150: 2.588\n",
      "Loss after mini-batch   200: 2.610\n",
      "Loss after mini-batch    50: 2.800\n",
      "Loss after mini-batch   100: 2.636\n",
      "Loss after mini-batch   150: 2.837\n",
      "Loss after mini-batch   200: 2.749\n",
      "Loss after mini-batch    50: 2.682\n",
      "Loss after mini-batch   100: 2.772\n",
      "Loss after mini-batch   150: 2.739\n",
      "Loss after mini-batch   200: 2.642\n",
      "Loss after mini-batch    50: 2.663\n",
      "Loss after mini-batch   100: 2.782\n",
      "Loss after mini-batch   150: 2.845\n",
      "Loss after mini-batch   200: 2.715\n",
      "Loss after mini-batch    50: 2.776\n",
      "Loss after mini-batch   100: 2.905\n",
      "Loss after mini-batch   150: 2.835\n",
      "Loss after mini-batch   200: 2.697\n",
      "Loss after mini-batch    50: 2.763\n",
      "Loss after mini-batch   100: 2.778\n",
      "Loss after mini-batch   150: 3.085\n",
      "Loss after mini-batch   200: 2.687\n",
      "Starting epoch 17\n",
      "Loss after mini-batch    50: 2.765\n",
      "Loss after mini-batch   100: 2.725\n",
      "Loss after mini-batch   150: 2.743\n",
      "Loss after mini-batch   200: 2.638\n",
      "Loss after mini-batch    50: 2.871\n",
      "Loss after mini-batch   100: 2.743\n",
      "Loss after mini-batch   150: 2.751\n",
      "Loss after mini-batch   200: 2.777\n",
      "Loss after mini-batch    50: 2.792\n",
      "Loss after mini-batch   100: 2.649\n",
      "Loss after mini-batch   150: 2.641\n",
      "Loss after mini-batch   200: 2.792\n",
      "Loss after mini-batch    50: 2.733\n",
      "Loss after mini-batch   100: 2.582\n",
      "Loss after mini-batch   150: 2.842\n",
      "Loss after mini-batch   200: 2.792\n",
      "Loss after mini-batch    50: 2.632\n",
      "Loss after mini-batch   100: 2.814\n",
      "Loss after mini-batch   150: 2.852\n",
      "Loss after mini-batch   200: 2.694\n",
      "Loss after mini-batch    50: 2.637\n",
      "Loss after mini-batch   100: 2.690\n",
      "Loss after mini-batch   150: 2.748\n",
      "Loss after mini-batch   200: 2.733\n",
      "Loss after mini-batch    50: 2.670\n",
      "Loss after mini-batch   100: 2.657\n",
      "Loss after mini-batch   150: 2.794\n",
      "Loss after mini-batch   200: 2.750\n",
      "Loss after mini-batch    50: 2.773\n",
      "Loss after mini-batch   100: 2.754\n",
      "Loss after mini-batch   150: 2.855\n",
      "Loss after mini-batch   200: 2.737\n",
      "Loss after mini-batch    50: 2.793\n",
      "Loss after mini-batch   100: 2.692\n",
      "Loss after mini-batch   150: 2.552\n",
      "Loss after mini-batch   200: 2.779\n",
      "Loss after mini-batch    50: 2.848\n",
      "Loss after mini-batch   100: 2.672\n",
      "Loss after mini-batch   150: 2.599\n",
      "Loss after mini-batch   200: 2.617\n",
      "Loss after mini-batch    50: 2.914\n",
      "Loss after mini-batch   100: 2.642\n",
      "Loss after mini-batch   150: 2.926\n",
      "Loss after mini-batch   200: 2.637\n",
      "Loss after mini-batch    50: 2.608\n",
      "Loss after mini-batch   100: 2.609\n",
      "Loss after mini-batch   150: 2.604\n",
      "Loss after mini-batch   200: 2.891\n",
      "Loss after mini-batch    50: 2.654\n",
      "Loss after mini-batch   100: 2.628\n",
      "Loss after mini-batch   150: 2.785\n",
      "Loss after mini-batch   200: 2.694\n",
      "Loss after mini-batch    50: 2.885\n",
      "Loss after mini-batch   100: 2.644\n",
      "Loss after mini-batch   150: 2.752\n",
      "Loss after mini-batch   200: 2.694\n",
      "Loss after mini-batch    50: 2.955\n",
      "Loss after mini-batch   100: 2.937\n",
      "Loss after mini-batch   150: 2.941\n",
      "Loss after mini-batch   200: 2.678\n",
      "Starting epoch 18\n",
      "Loss after mini-batch    50: 2.730\n",
      "Loss after mini-batch   100: 2.699\n",
      "Loss after mini-batch   150: 2.659\n",
      "Loss after mini-batch   200: 2.750\n",
      "Loss after mini-batch    50: 2.857\n",
      "Loss after mini-batch   100: 2.834\n",
      "Loss after mini-batch   150: 2.765\n",
      "Loss after mini-batch   200: 2.680\n",
      "Loss after mini-batch    50: 2.639\n",
      "Loss after mini-batch   100: 2.827\n",
      "Loss after mini-batch   150: 2.766\n",
      "Loss after mini-batch   200: 2.676\n",
      "Loss after mini-batch    50: 2.552\n",
      "Loss after mini-batch   100: 2.737\n",
      "Loss after mini-batch   150: 2.844\n",
      "Loss after mini-batch   200: 2.710\n",
      "Loss after mini-batch    50: 2.665\n",
      "Loss after mini-batch   100: 2.808\n",
      "Loss after mini-batch   150: 2.789\n",
      "Loss after mini-batch   200: 2.660\n",
      "Loss after mini-batch    50: 2.723\n",
      "Loss after mini-batch   100: 2.813\n",
      "Loss after mini-batch   150: 2.754\n",
      "Loss after mini-batch   200: 2.784\n",
      "Loss after mini-batch    50: 2.947\n",
      "Loss after mini-batch   100: 2.710\n",
      "Loss after mini-batch   150: 2.799\n",
      "Loss after mini-batch   200: 2.770\n",
      "Loss after mini-batch    50: 2.767\n",
      "Loss after mini-batch   100: 2.866\n",
      "Loss after mini-batch   150: 2.786\n",
      "Loss after mini-batch   200: 2.755\n",
      "Loss after mini-batch    50: 2.754\n",
      "Loss after mini-batch   100: 2.746\n",
      "Loss after mini-batch   150: 2.670\n",
      "Loss after mini-batch   200: 2.773\n",
      "Loss after mini-batch    50: 2.583\n",
      "Loss after mini-batch   100: 2.809\n",
      "Loss after mini-batch   150: 2.681\n",
      "Loss after mini-batch   200: 2.706\n",
      "Loss after mini-batch    50: 2.652\n",
      "Loss after mini-batch   100: 2.763\n",
      "Loss after mini-batch   150: 2.828\n",
      "Loss after mini-batch   200: 2.811\n",
      "Loss after mini-batch    50: 2.689\n",
      "Loss after mini-batch   100: 2.611\n",
      "Loss after mini-batch   150: 2.752\n",
      "Loss after mini-batch   200: 2.746\n",
      "Loss after mini-batch    50: 2.692\n",
      "Loss after mini-batch   100: 2.693\n",
      "Loss after mini-batch   150: 2.727\n",
      "Loss after mini-batch   200: 2.591\n",
      "Loss after mini-batch    50: 2.887\n",
      "Loss after mini-batch   100: 2.618\n",
      "Loss after mini-batch   150: 2.777\n",
      "Loss after mini-batch   200: 2.742\n",
      "Loss after mini-batch    50: 2.811\n",
      "Loss after mini-batch   100: 2.731\n",
      "Loss after mini-batch   150: 2.944\n",
      "Loss after mini-batch   200: 2.844\n",
      "Starting epoch 19\n",
      "Loss after mini-batch    50: 2.573\n",
      "Loss after mini-batch   100: 2.817\n",
      "Loss after mini-batch   150: 2.731\n",
      "Loss after mini-batch   200: 2.744\n",
      "Loss after mini-batch    50: 2.744\n",
      "Loss after mini-batch   100: 2.739\n",
      "Loss after mini-batch   150: 2.751\n",
      "Loss after mini-batch   200: 2.730\n",
      "Loss after mini-batch    50: 2.781\n",
      "Loss after mini-batch   100: 2.800\n",
      "Loss after mini-batch   150: 2.728\n",
      "Loss after mini-batch   200: 2.730\n",
      "Loss after mini-batch    50: 2.711\n",
      "Loss after mini-batch   100: 2.839\n",
      "Loss after mini-batch   150: 2.711\n",
      "Loss after mini-batch   200: 2.638\n",
      "Loss after mini-batch    50: 2.677\n",
      "Loss after mini-batch   100: 2.775\n",
      "Loss after mini-batch   150: 2.690\n",
      "Loss after mini-batch   200: 2.803\n",
      "Loss after mini-batch    50: 2.912\n",
      "Loss after mini-batch   100: 2.805\n",
      "Loss after mini-batch   150: 2.625\n",
      "Loss after mini-batch   200: 2.675\n",
      "Loss after mini-batch    50: 2.724\n",
      "Loss after mini-batch   100: 2.767\n",
      "Loss after mini-batch   150: 2.791\n",
      "Loss after mini-batch   200: 2.719\n",
      "Loss after mini-batch    50: 2.800\n",
      "Loss after mini-batch   100: 2.788\n",
      "Loss after mini-batch   150: 2.750\n",
      "Loss after mini-batch   200: 2.662\n",
      "Loss after mini-batch    50: 2.624\n",
      "Loss after mini-batch   100: 2.612\n",
      "Loss after mini-batch   150: 2.757\n",
      "Loss after mini-batch   200: 2.848\n",
      "Loss after mini-batch    50: 2.730\n",
      "Loss after mini-batch   100: 2.592\n",
      "Loss after mini-batch   150: 2.712\n",
      "Loss after mini-batch   200: 2.646\n",
      "Loss after mini-batch    50: 2.725\n",
      "Loss after mini-batch   100: 2.690\n",
      "Loss after mini-batch   150: 2.860\n",
      "Loss after mini-batch   200: 2.788\n",
      "Loss after mini-batch    50: 2.760\n",
      "Loss after mini-batch   100: 2.682\n",
      "Loss after mini-batch   150: 2.661\n",
      "Loss after mini-batch   200: 2.662\n",
      "Loss after mini-batch    50: 2.618\n",
      "Loss after mini-batch   100: 2.893\n",
      "Loss after mini-batch   150: 2.746\n",
      "Loss after mini-batch   200: 2.604\n",
      "Loss after mini-batch    50: 2.766\n",
      "Loss after mini-batch   100: 2.800\n",
      "Loss after mini-batch   150: 2.756\n",
      "Loss after mini-batch   200: 2.748\n",
      "Loss after mini-batch    50: 2.943\n",
      "Loss after mini-batch   100: 2.858\n",
      "Loss after mini-batch   150: 2.864\n",
      "Loss after mini-batch   200: 2.671\n",
      "Starting epoch 20\n",
      "Loss after mini-batch    50: 2.784\n",
      "Loss after mini-batch   100: 2.586\n",
      "Loss after mini-batch   150: 2.718\n",
      "Loss after mini-batch   200: 2.744\n",
      "Loss after mini-batch    50: 2.784\n",
      "Loss after mini-batch   100: 2.857\n",
      "Loss after mini-batch   150: 2.790\n",
      "Loss after mini-batch   200: 2.588\n",
      "Loss after mini-batch    50: 2.663\n",
      "Loss after mini-batch   100: 2.837\n",
      "Loss after mini-batch   150: 2.713\n",
      "Loss after mini-batch   200: 2.770\n",
      "Loss after mini-batch    50: 2.749\n",
      "Loss after mini-batch   100: 2.767\n",
      "Loss after mini-batch   150: 2.717\n",
      "Loss after mini-batch   200: 2.707\n",
      "Loss after mini-batch    50: 2.631\n",
      "Loss after mini-batch   100: 2.777\n",
      "Loss after mini-batch   150: 2.631\n",
      "Loss after mini-batch   200: 2.745\n",
      "Loss after mini-batch    50: 2.801\n",
      "Loss after mini-batch   100: 2.894\n",
      "Loss after mini-batch   150: 2.789\n",
      "Loss after mini-batch   200: 2.629\n",
      "Loss after mini-batch    50: 2.563\n",
      "Loss after mini-batch   100: 2.773\n",
      "Loss after mini-batch   150: 2.898\n",
      "Loss after mini-batch   200: 2.685\n",
      "Loss after mini-batch    50: 2.794\n",
      "Loss after mini-batch   100: 2.838\n",
      "Loss after mini-batch   150: 2.860\n",
      "Loss after mini-batch   200: 2.674\n",
      "Loss after mini-batch    50: 2.672\n",
      "Loss after mini-batch   100: 2.722\n",
      "Loss after mini-batch   150: 2.767\n",
      "Loss after mini-batch   200: 2.660\n",
      "Loss after mini-batch    50: 2.676\n",
      "Loss after mini-batch   100: 2.627\n",
      "Loss after mini-batch   150: 2.622\n",
      "Loss after mini-batch   200: 2.758\n",
      "Loss after mini-batch    50: 2.777\n",
      "Loss after mini-batch   100: 2.764\n",
      "Loss after mini-batch   150: 2.768\n",
      "Loss after mini-batch   200: 2.643\n",
      "Loss after mini-batch    50: 2.601\n",
      "Loss after mini-batch   100: 2.730\n",
      "Loss after mini-batch   150: 2.729\n",
      "Loss after mini-batch   200: 2.725\n",
      "Loss after mini-batch    50: 2.716\n",
      "Loss after mini-batch   100: 2.872\n",
      "Loss after mini-batch   150: 2.673\n",
      "Loss after mini-batch   200: 2.613\n",
      "Loss after mini-batch    50: 2.914\n",
      "Loss after mini-batch   100: 2.710\n",
      "Loss after mini-batch   150: 2.752\n",
      "Loss after mini-batch   200: 2.761\n",
      "Loss after mini-batch    50: 2.826\n",
      "Loss after mini-batch   100: 2.924\n",
      "Loss after mini-batch   150: 2.781\n",
      "Loss after mini-batch   200: 2.877\n",
      "Starting epoch 21\n",
      "Loss after mini-batch    50: 2.757\n",
      "Loss after mini-batch   100: 2.657\n",
      "Loss after mini-batch   150: 2.742\n",
      "Loss after mini-batch   200: 2.799\n",
      "Loss after mini-batch    50: 2.792\n",
      "Loss after mini-batch   100: 2.607\n",
      "Loss after mini-batch   150: 2.875\n",
      "Loss after mini-batch   200: 2.824\n",
      "Loss after mini-batch    50: 2.760\n",
      "Loss after mini-batch   100: 2.760\n",
      "Loss after mini-batch   150: 2.777\n",
      "Loss after mini-batch   200: 2.639\n",
      "Loss after mini-batch    50: 2.683\n",
      "Loss after mini-batch   100: 2.884\n",
      "Loss after mini-batch   150: 2.680\n",
      "Loss after mini-batch   200: 2.730\n",
      "Loss after mini-batch    50: 2.555\n",
      "Loss after mini-batch   100: 2.811\n",
      "Loss after mini-batch   150: 2.886\n",
      "Loss after mini-batch   200: 2.777\n",
      "Loss after mini-batch    50: 2.726\n",
      "Loss after mini-batch   100: 2.667\n",
      "Loss after mini-batch   150: 2.646\n",
      "Loss after mini-batch   200: 2.755\n",
      "Loss after mini-batch    50: 2.685\n",
      "Loss after mini-batch   100: 2.761\n",
      "Loss after mini-batch   150: 2.773\n",
      "Loss after mini-batch   200: 2.853\n",
      "Loss after mini-batch    50: 2.858\n",
      "Loss after mini-batch   100: 2.732\n",
      "Loss after mini-batch   150: 2.740\n",
      "Loss after mini-batch   200: 2.711\n",
      "Loss after mini-batch    50: 2.827\n",
      "Loss after mini-batch   100: 2.588\n",
      "Loss after mini-batch   150: 2.614\n",
      "Loss after mini-batch   200: 2.831\n",
      "Loss after mini-batch    50: 2.670\n",
      "Loss after mini-batch   100: 2.595\n",
      "Loss after mini-batch   150: 2.647\n",
      "Loss after mini-batch   200: 2.841\n",
      "Loss after mini-batch    50: 2.627\n",
      "Loss after mini-batch   100: 2.713\n",
      "Loss after mini-batch   150: 2.776\n",
      "Loss after mini-batch   200: 2.799\n",
      "Loss after mini-batch    50: 2.745\n",
      "Loss after mini-batch   100: 2.630\n",
      "Loss after mini-batch   150: 2.699\n",
      "Loss after mini-batch   200: 2.666\n",
      "Loss after mini-batch    50: 2.673\n",
      "Loss after mini-batch   100: 2.649\n",
      "Loss after mini-batch   150: 2.564\n",
      "Loss after mini-batch   200: 2.757\n",
      "Loss after mini-batch    50: 2.711\n",
      "Loss after mini-batch   100: 2.701\n",
      "Loss after mini-batch   150: 2.767\n",
      "Loss after mini-batch   200: 2.571\n",
      "Loss after mini-batch    50: 2.797\n",
      "Loss after mini-batch   100: 2.958\n",
      "Loss after mini-batch   150: 2.750\n",
      "Loss after mini-batch   200: 2.923\n",
      "Starting epoch 22\n",
      "Loss after mini-batch    50: 2.764\n",
      "Loss after mini-batch   100: 2.641\n",
      "Loss after mini-batch   150: 2.752\n",
      "Loss after mini-batch   200: 2.715\n",
      "Loss after mini-batch    50: 2.723\n",
      "Loss after mini-batch   100: 2.647\n",
      "Loss after mini-batch   150: 2.790\n",
      "Loss after mini-batch   200: 2.811\n",
      "Loss after mini-batch    50: 2.890\n",
      "Loss after mini-batch   100: 2.787\n",
      "Loss after mini-batch   150: 2.755\n",
      "Loss after mini-batch   200: 2.687\n",
      "Loss after mini-batch    50: 2.674\n",
      "Loss after mini-batch   100: 2.827\n",
      "Loss after mini-batch   150: 2.723\n",
      "Loss after mini-batch   200: 2.718\n",
      "Loss after mini-batch    50: 2.608\n",
      "Loss after mini-batch   100: 2.780\n",
      "Loss after mini-batch   150: 2.691\n",
      "Loss after mini-batch   200: 2.790\n",
      "Loss after mini-batch    50: 2.827\n",
      "Loss after mini-batch   100: 2.743\n",
      "Loss after mini-batch   150: 2.646\n",
      "Loss after mini-batch   200: 2.708\n",
      "Loss after mini-batch    50: 2.698\n",
      "Loss after mini-batch   100: 2.712\n",
      "Loss after mini-batch   150: 2.808\n",
      "Loss after mini-batch   200: 2.842\n",
      "Loss after mini-batch    50: 2.826\n",
      "Loss after mini-batch   100: 2.760\n",
      "Loss after mini-batch   150: 2.754\n",
      "Loss after mini-batch   200: 2.704\n",
      "Loss after mini-batch    50: 2.851\n",
      "Loss after mini-batch   100: 2.744\n",
      "Loss after mini-batch   150: 2.755\n",
      "Loss after mini-batch   200: 2.686\n",
      "Loss after mini-batch    50: 2.606\n",
      "Loss after mini-batch   100: 2.753\n",
      "Loss after mini-batch   150: 2.667\n",
      "Loss after mini-batch   200: 2.688\n",
      "Loss after mini-batch    50: 2.727\n",
      "Loss after mini-batch   100: 2.723\n",
      "Loss after mini-batch   150: 2.758\n",
      "Loss after mini-batch   200: 2.882\n",
      "Loss after mini-batch    50: 2.811\n",
      "Loss after mini-batch   100: 2.665\n",
      "Loss after mini-batch   150: 2.725\n",
      "Loss after mini-batch   200: 2.635\n",
      "Loss after mini-batch    50: 2.783\n",
      "Loss after mini-batch   100: 2.733\n",
      "Loss after mini-batch   150: 2.754\n",
      "Loss after mini-batch   200: 2.641\n",
      "Loss after mini-batch    50: 2.740\n",
      "Loss after mini-batch   100: 2.618\n",
      "Loss after mini-batch   150: 2.842\n",
      "Loss after mini-batch   200: 2.871\n",
      "Loss after mini-batch    50: 2.963\n",
      "Loss after mini-batch   100: 2.841\n",
      "Loss after mini-batch   150: 2.691\n",
      "Loss after mini-batch   200: 2.857\n",
      "Starting epoch 23\n",
      "Loss after mini-batch    50: 2.586\n",
      "Loss after mini-batch   100: 2.768\n",
      "Loss after mini-batch   150: 2.829\n",
      "Loss after mini-batch   200: 2.693\n",
      "Loss after mini-batch    50: 2.745\n",
      "Loss after mini-batch   100: 2.809\n",
      "Loss after mini-batch   150: 2.705\n",
      "Loss after mini-batch   200: 2.734\n",
      "Loss after mini-batch    50: 2.762\n",
      "Loss after mini-batch   100: 2.712\n",
      "Loss after mini-batch   150: 2.755\n",
      "Loss after mini-batch   200: 2.755\n",
      "Loss after mini-batch    50: 2.762\n",
      "Loss after mini-batch   100: 2.637\n",
      "Loss after mini-batch   150: 2.818\n",
      "Loss after mini-batch   200: 2.739\n",
      "Loss after mini-batch    50: 2.702\n",
      "Loss after mini-batch   100: 2.667\n",
      "Loss after mini-batch   150: 2.736\n",
      "Loss after mini-batch   200: 2.817\n",
      "Loss after mini-batch    50: 2.667\n",
      "Loss after mini-batch   100: 2.740\n",
      "Loss after mini-batch   150: 2.722\n",
      "Loss after mini-batch   200: 2.723\n",
      "Loss after mini-batch    50: 2.703\n",
      "Loss after mini-batch   100: 2.661\n",
      "Loss after mini-batch   150: 2.958\n",
      "Loss after mini-batch   200: 2.601\n",
      "Loss after mini-batch    50: 2.754\n",
      "Loss after mini-batch   100: 2.854\n",
      "Loss after mini-batch   150: 2.878\n",
      "Loss after mini-batch   200: 2.698\n",
      "Loss after mini-batch    50: 2.634\n",
      "Loss after mini-batch   100: 2.601\n",
      "Loss after mini-batch   150: 2.920\n",
      "Loss after mini-batch   200: 2.637\n",
      "Loss after mini-batch    50: 2.691\n",
      "Loss after mini-batch   100: 2.741\n",
      "Loss after mini-batch   150: 2.581\n",
      "Loss after mini-batch   200: 2.624\n",
      "Loss after mini-batch    50: 2.905\n",
      "Loss after mini-batch   100: 2.653\n",
      "Loss after mini-batch   150: 2.689\n",
      "Loss after mini-batch   200: 2.895\n",
      "Loss after mini-batch    50: 2.654\n",
      "Loss after mini-batch   100: 2.624\n",
      "Loss after mini-batch   150: 2.778\n",
      "Loss after mini-batch   200: 2.694\n",
      "Loss after mini-batch    50: 2.809\n",
      "Loss after mini-batch   100: 2.631\n",
      "Loss after mini-batch   150: 2.628\n",
      "Loss after mini-batch   200: 2.776\n",
      "Loss after mini-batch    50: 2.764\n",
      "Loss after mini-batch   100: 2.640\n",
      "Loss after mini-batch   150: 2.738\n",
      "Loss after mini-batch   200: 2.748\n",
      "Loss after mini-batch    50: 2.790\n",
      "Loss after mini-batch   100: 2.992\n",
      "Loss after mini-batch   150: 2.899\n",
      "Loss after mini-batch   200: 2.746\n",
      "Starting epoch 24\n",
      "Loss after mini-batch    50: 2.758\n",
      "Loss after mini-batch   100: 2.770\n",
      "Loss after mini-batch   150: 2.688\n",
      "Loss after mini-batch   200: 2.599\n",
      "Loss after mini-batch    50: 2.765\n",
      "Loss after mini-batch   100: 2.863\n",
      "Loss after mini-batch   150: 2.709\n",
      "Loss after mini-batch   200: 2.704\n",
      "Loss after mini-batch    50: 2.772\n",
      "Loss after mini-batch   100: 2.719\n",
      "Loss after mini-batch   150: 2.719\n",
      "Loss after mini-batch   200: 2.726\n",
      "Loss after mini-batch    50: 2.676\n",
      "Loss after mini-batch   100: 2.841\n",
      "Loss after mini-batch   150: 2.723\n",
      "Loss after mini-batch   200: 2.812\n",
      "Loss after mini-batch    50: 2.807\n",
      "Loss after mini-batch   100: 2.846\n",
      "Loss after mini-batch   150: 2.589\n",
      "Loss after mini-batch   200: 2.649\n",
      "Loss after mini-batch    50: 2.799\n",
      "Loss after mini-batch   100: 2.743\n",
      "Loss after mini-batch   150: 2.611\n",
      "Loss after mini-batch   200: 2.777\n",
      "Loss after mini-batch    50: 2.799\n",
      "Loss after mini-batch   100: 2.664\n",
      "Loss after mini-batch   150: 2.854\n",
      "Loss after mini-batch   200: 2.675\n",
      "Loss after mini-batch    50: 2.932\n",
      "Loss after mini-batch   100: 2.597\n",
      "Loss after mini-batch   150: 2.757\n",
      "Loss after mini-batch   200: 2.754\n",
      "Loss after mini-batch    50: 2.728\n",
      "Loss after mini-batch   100: 2.701\n",
      "Loss after mini-batch   150: 2.673\n",
      "Loss after mini-batch   200: 2.713\n",
      "Loss after mini-batch    50: 2.764\n",
      "Loss after mini-batch   100: 2.651\n",
      "Loss after mini-batch   150: 2.608\n",
      "Loss after mini-batch   200: 2.614\n",
      "Loss after mini-batch    50: 2.621\n",
      "Loss after mini-batch   100: 2.802\n",
      "Loss after mini-batch   150: 2.812\n",
      "Loss after mini-batch   200: 2.670\n",
      "Loss after mini-batch    50: 2.737\n",
      "Loss after mini-batch   100: 2.553\n",
      "Loss after mini-batch   150: 2.724\n",
      "Loss after mini-batch   200: 2.582\n",
      "Loss after mini-batch    50: 2.719\n",
      "Loss after mini-batch   100: 2.668\n",
      "Loss after mini-batch   150: 2.716\n",
      "Loss after mini-batch   200: 2.829\n",
      "Loss after mini-batch    50: 2.555\n",
      "Loss after mini-batch   100: 2.916\n",
      "Loss after mini-batch   150: 2.828\n",
      "Loss after mini-batch   200: 2.768\n",
      "Loss after mini-batch    50: 2.934\n",
      "Loss after mini-batch   100: 2.832\n",
      "Loss after mini-batch   150: 2.744\n",
      "Loss after mini-batch   200: 2.854\n",
      "Starting epoch 25\n",
      "Loss after mini-batch    50: 2.794\n",
      "Loss after mini-batch   100: 2.628\n",
      "Loss after mini-batch   150: 2.803\n",
      "Loss after mini-batch   200: 2.603\n",
      "Loss after mini-batch    50: 2.791\n",
      "Loss after mini-batch   100: 2.843\n",
      "Loss after mini-batch   150: 2.707\n",
      "Loss after mini-batch   200: 2.728\n",
      "Loss after mini-batch    50: 2.699\n",
      "Loss after mini-batch   100: 2.858\n",
      "Loss after mini-batch   150: 2.784\n",
      "Loss after mini-batch   200: 2.604\n",
      "Loss after mini-batch    50: 2.862\n",
      "Loss after mini-batch   100: 2.642\n",
      "Loss after mini-batch   150: 2.670\n",
      "Loss after mini-batch   200: 2.800\n",
      "Loss after mini-batch    50: 2.836\n",
      "Loss after mini-batch   100: 2.557\n",
      "Loss after mini-batch   150: 2.679\n",
      "Loss after mini-batch   200: 2.749\n",
      "Loss after mini-batch    50: 2.795\n",
      "Loss after mini-batch   100: 2.688\n",
      "Loss after mini-batch   150: 2.870\n",
      "Loss after mini-batch   200: 2.680\n",
      "Loss after mini-batch    50: 2.679\n",
      "Loss after mini-batch   100: 2.821\n",
      "Loss after mini-batch   150: 2.793\n",
      "Loss after mini-batch   200: 2.829\n",
      "Loss after mini-batch    50: 2.749\n",
      "Loss after mini-batch   100: 2.764\n",
      "Loss after mini-batch   150: 2.820\n",
      "Loss after mini-batch   200: 2.763\n",
      "Loss after mini-batch    50: 2.575\n",
      "Loss after mini-batch   100: 2.709\n",
      "Loss after mini-batch   150: 2.861\n",
      "Loss after mini-batch   200: 2.788\n",
      "Loss after mini-batch    50: 2.681\n",
      "Loss after mini-batch   100: 2.691\n",
      "Loss after mini-batch   150: 2.654\n",
      "Loss after mini-batch   200: 2.716\n",
      "Loss after mini-batch    50: 2.536\n",
      "Loss after mini-batch   100: 2.865\n",
      "Loss after mini-batch   150: 2.738\n",
      "Loss after mini-batch   200: 2.782\n",
      "Loss after mini-batch    50: 2.702\n",
      "Loss after mini-batch   100: 2.625\n",
      "Loss after mini-batch   150: 2.654\n",
      "Loss after mini-batch   200: 2.626\n",
      "Loss after mini-batch    50: 2.675\n",
      "Loss after mini-batch   100: 2.684\n",
      "Loss after mini-batch   150: 2.590\n",
      "Loss after mini-batch   200: 2.826\n",
      "Loss after mini-batch    50: 2.695\n",
      "Loss after mini-batch   100: 2.596\n",
      "Loss after mini-batch   150: 2.739\n",
      "Loss after mini-batch   200: 2.806\n",
      "Loss after mini-batch    50: 2.599\n",
      "Loss after mini-batch   100: 2.757\n",
      "Loss after mini-batch   150: 2.820\n",
      "Loss after mini-batch   200: 2.975\n",
      "Starting epoch 26\n",
      "Loss after mini-batch    50: 2.738\n",
      "Loss after mini-batch   100: 2.687\n",
      "Loss after mini-batch   150: 2.781\n",
      "Loss after mini-batch   200: 2.638\n",
      "Loss after mini-batch    50: 2.692\n",
      "Loss after mini-batch   100: 2.847\n",
      "Loss after mini-batch   150: 2.696\n",
      "Loss after mini-batch   200: 2.796\n",
      "Loss after mini-batch    50: 2.781\n",
      "Loss after mini-batch   100: 2.643\n",
      "Loss after mini-batch   150: 2.855\n",
      "Loss after mini-batch   200: 2.737\n",
      "Loss after mini-batch    50: 2.731\n",
      "Loss after mini-batch   100: 2.770\n",
      "Loss after mini-batch   150: 2.695\n",
      "Loss after mini-batch   200: 2.693\n",
      "Loss after mini-batch    50: 2.637\n",
      "Loss after mini-batch   100: 2.684\n",
      "Loss after mini-batch   150: 2.676\n",
      "Loss after mini-batch   200: 2.719\n",
      "Loss after mini-batch    50: 2.657\n",
      "Loss after mini-batch   100: 2.790\n",
      "Loss after mini-batch   150: 2.749\n",
      "Loss after mini-batch   200: 2.730\n",
      "Loss after mini-batch    50: 2.845\n",
      "Loss after mini-batch   100: 2.694\n",
      "Loss after mini-batch   150: 2.763\n",
      "Loss after mini-batch   200: 2.698\n",
      "Loss after mini-batch    50: 2.823\n",
      "Loss after mini-batch   100: 2.740\n",
      "Loss after mini-batch   150: 2.687\n",
      "Loss after mini-batch   200: 2.830\n",
      "Loss after mini-batch    50: 2.525\n",
      "Loss after mini-batch   100: 2.857\n",
      "Loss after mini-batch   150: 3.020\n",
      "Loss after mini-batch   200: 2.616\n",
      "Loss after mini-batch    50: 2.654\n",
      "Loss after mini-batch   100: 2.768\n",
      "Loss after mini-batch   150: 2.577\n",
      "Loss after mini-batch   200: 2.659\n",
      "Loss after mini-batch    50: 2.668\n",
      "Loss after mini-batch   100: 2.785\n",
      "Loss after mini-batch   150: 2.770\n",
      "Loss after mini-batch   200: 2.697\n",
      "Loss after mini-batch    50: 2.817\n",
      "Loss after mini-batch   100: 2.586\n",
      "Loss after mini-batch   150: 2.669\n",
      "Loss after mini-batch   200: 2.638\n",
      "Loss after mini-batch    50: 2.668\n",
      "Loss after mini-batch   100: 2.902\n",
      "Loss after mini-batch   150: 2.551\n",
      "Loss after mini-batch   200: 2.676\n",
      "Loss after mini-batch    50: 2.754\n",
      "Loss after mini-batch   100: 2.599\n",
      "Loss after mini-batch   150: 2.752\n",
      "Loss after mini-batch   200: 2.740\n",
      "Loss after mini-batch    50: 2.831\n",
      "Loss after mini-batch   100: 2.651\n",
      "Loss after mini-batch   150: 2.923\n",
      "Loss after mini-batch   200: 2.894\n",
      "Starting epoch 27\n",
      "Loss after mini-batch    50: 2.774\n",
      "Loss after mini-batch   100: 2.781\n",
      "Loss after mini-batch   150: 2.588\n",
      "Loss after mini-batch   200: 2.656\n",
      "Loss after mini-batch    50: 2.990\n",
      "Loss after mini-batch   100: 2.702\n",
      "Loss after mini-batch   150: 2.647\n",
      "Loss after mini-batch   200: 2.655\n",
      "Loss after mini-batch    50: 2.686\n",
      "Loss after mini-batch   100: 2.708\n",
      "Loss after mini-batch   150: 2.897\n",
      "Loss after mini-batch   200: 2.683\n",
      "Loss after mini-batch    50: 2.716\n",
      "Loss after mini-batch   100: 2.684\n",
      "Loss after mini-batch   150: 2.720\n",
      "Loss after mini-batch   200: 2.792\n",
      "Loss after mini-batch    50: 2.757\n",
      "Loss after mini-batch   100: 2.721\n",
      "Loss after mini-batch   150: 2.600\n",
      "Loss after mini-batch   200: 2.680\n",
      "Loss after mini-batch    50: 2.647\n",
      "Loss after mini-batch   100: 2.957\n",
      "Loss after mini-batch   150: 2.656\n",
      "Loss after mini-batch   200: 2.616\n",
      "Loss after mini-batch    50: 2.782\n",
      "Loss after mini-batch   100: 2.752\n",
      "Loss after mini-batch   150: 2.823\n",
      "Loss after mini-batch   200: 2.693\n",
      "Loss after mini-batch    50: 2.589\n",
      "Loss after mini-batch   100: 2.872\n",
      "Loss after mini-batch   150: 2.829\n",
      "Loss after mini-batch   200: 2.696\n",
      "Loss after mini-batch    50: 2.593\n",
      "Loss after mini-batch   100: 2.737\n",
      "Loss after mini-batch   150: 2.691\n",
      "Loss after mini-batch   200: 2.743\n",
      "Loss after mini-batch    50: 2.626\n",
      "Loss after mini-batch   100: 2.645\n",
      "Loss after mini-batch   150: 2.803\n",
      "Loss after mini-batch   200: 2.717\n",
      "Loss after mini-batch    50: 2.988\n",
      "Loss after mini-batch   100: 2.748\n",
      "Loss after mini-batch   150: 2.731\n",
      "Loss after mini-batch   200: 2.667\n",
      "Loss after mini-batch    50: 2.656\n",
      "Loss after mini-batch   100: 2.715\n",
      "Loss after mini-batch   150: 2.661\n",
      "Loss after mini-batch   200: 2.665\n",
      "Loss after mini-batch    50: 2.673\n",
      "Loss after mini-batch   100: 2.690\n",
      "Loss after mini-batch   150: 2.752\n",
      "Loss after mini-batch   200: 2.644\n",
      "Loss after mini-batch    50: 2.773\n",
      "Loss after mini-batch   100: 2.648\n",
      "Loss after mini-batch   150: 2.714\n",
      "Loss after mini-batch   200: 2.769\n",
      "Loss after mini-batch    50: 2.832\n",
      "Loss after mini-batch   100: 2.895\n",
      "Loss after mini-batch   150: 2.891\n",
      "Loss after mini-batch   200: 2.799\n",
      "Starting epoch 28\n",
      "Loss after mini-batch    50: 2.644\n",
      "Loss after mini-batch   100: 2.686\n",
      "Loss after mini-batch   150: 2.746\n",
      "Loss after mini-batch   200: 2.688\n",
      "Loss after mini-batch    50: 2.762\n",
      "Loss after mini-batch   100: 2.804\n",
      "Loss after mini-batch   150: 2.747\n",
      "Loss after mini-batch   200: 2.614\n",
      "Loss after mini-batch    50: 2.883\n",
      "Loss after mini-batch   100: 2.697\n",
      "Loss after mini-batch   150: 2.659\n",
      "Loss after mini-batch   200: 2.695\n",
      "Loss after mini-batch    50: 2.677\n",
      "Loss after mini-batch   100: 2.706\n",
      "Loss after mini-batch   150: 2.729\n",
      "Loss after mini-batch   200: 2.790\n",
      "Loss after mini-batch    50: 2.603\n",
      "Loss after mini-batch   100: 2.682\n",
      "Loss after mini-batch   150: 2.775\n",
      "Loss after mini-batch   200: 2.806\n",
      "Loss after mini-batch    50: 2.792\n",
      "Loss after mini-batch   100: 2.859\n",
      "Loss after mini-batch   150: 2.602\n",
      "Loss after mini-batch   200: 2.637\n",
      "Loss after mini-batch    50: 2.768\n",
      "Loss after mini-batch   100: 2.793\n",
      "Loss after mini-batch   150: 2.818\n",
      "Loss after mini-batch   200: 2.715\n",
      "Loss after mini-batch    50: 2.866\n",
      "Loss after mini-batch   100: 2.708\n",
      "Loss after mini-batch   150: 2.763\n",
      "Loss after mini-batch   200: 2.791\n",
      "Loss after mini-batch    50: 2.780\n",
      "Loss after mini-batch   100: 2.694\n",
      "Loss after mini-batch   150: 2.661\n",
      "Loss after mini-batch   200: 2.688\n",
      "Loss after mini-batch    50: 2.678\n",
      "Loss after mini-batch   100: 2.622\n",
      "Loss after mini-batch   150: 2.679\n",
      "Loss after mini-batch   200: 2.702\n",
      "Loss after mini-batch    50: 2.750\n",
      "Loss after mini-batch   100: 2.737\n",
      "Loss after mini-batch   150: 2.764\n",
      "Loss after mini-batch   200: 2.815\n",
      "Loss after mini-batch    50: 2.743\n",
      "Loss after mini-batch   100: 2.662\n",
      "Loss after mini-batch   150: 2.624\n",
      "Loss after mini-batch   200: 2.636\n",
      "Loss after mini-batch    50: 2.793\n",
      "Loss after mini-batch   100: 2.618\n",
      "Loss after mini-batch   150: 2.735\n",
      "Loss after mini-batch   200: 2.811\n",
      "Loss after mini-batch    50: 2.743\n",
      "Loss after mini-batch   100: 2.676\n",
      "Loss after mini-batch   150: 2.794\n",
      "Loss after mini-batch   200: 2.804\n",
      "Loss after mini-batch    50: 2.718\n",
      "Loss after mini-batch   100: 2.826\n",
      "Loss after mini-batch   150: 2.848\n",
      "Loss after mini-batch   200: 3.041\n",
      "Starting epoch 29\n",
      "Loss after mini-batch    50: 2.688\n",
      "Loss after mini-batch   100: 2.618\n",
      "Loss after mini-batch   150: 2.838\n",
      "Loss after mini-batch   200: 2.680\n",
      "Loss after mini-batch    50: 2.648\n",
      "Loss after mini-batch   100: 2.777\n",
      "Loss after mini-batch   150: 2.768\n",
      "Loss after mini-batch   200: 2.871\n",
      "Loss after mini-batch    50: 2.767\n",
      "Loss after mini-batch   100: 2.675\n",
      "Loss after mini-batch   150: 2.810\n",
      "Loss after mini-batch   200: 2.708\n",
      "Loss after mini-batch    50: 2.743\n",
      "Loss after mini-batch   100: 2.762\n",
      "Loss after mini-batch   150: 2.719\n",
      "Loss after mini-batch   200: 2.645\n",
      "Loss after mini-batch    50: 2.656\n",
      "Loss after mini-batch   100: 2.734\n",
      "Loss after mini-batch   150: 2.581\n",
      "Loss after mini-batch   200: 2.820\n",
      "Loss after mini-batch    50: 2.802\n",
      "Loss after mini-batch   100: 2.707\n",
      "Loss after mini-batch   150: 2.704\n",
      "Loss after mini-batch   200: 2.679\n",
      "Loss after mini-batch    50: 2.776\n",
      "Loss after mini-batch   100: 2.683\n",
      "Loss after mini-batch   150: 2.951\n",
      "Loss after mini-batch   200: 2.607\n",
      "Loss after mini-batch    50: 2.871\n",
      "Loss after mini-batch   100: 2.785\n",
      "Loss after mini-batch   150: 2.739\n",
      "Loss after mini-batch   200: 2.750\n",
      "Loss after mini-batch    50: 2.624\n",
      "Loss after mini-batch   100: 2.902\n",
      "Loss after mini-batch   150: 2.728\n",
      "Loss after mini-batch   200: 2.672\n",
      "Loss after mini-batch    50: 2.648\n",
      "Loss after mini-batch   100: 2.670\n",
      "Loss after mini-batch   150: 2.677\n",
      "Loss after mini-batch   200: 2.667\n",
      "Loss after mini-batch    50: 2.803\n",
      "Loss after mini-batch   100: 2.705\n",
      "Loss after mini-batch   150: 2.874\n",
      "Loss after mini-batch   200: 2.660\n",
      "Loss after mini-batch    50: 2.697\n",
      "Loss after mini-batch   100: 2.669\n",
      "Loss after mini-batch   150: 2.679\n",
      "Loss after mini-batch   200: 2.740\n",
      "Loss after mini-batch    50: 2.835\n",
      "Loss after mini-batch   100: 2.618\n",
      "Loss after mini-batch   150: 2.597\n",
      "Loss after mini-batch   200: 2.653\n",
      "Loss after mini-batch    50: 2.879\n",
      "Loss after mini-batch   100: 2.747\n",
      "Loss after mini-batch   150: 2.702\n",
      "Loss after mini-batch   200: 2.702\n",
      "Loss after mini-batch    50: 2.707\n",
      "Loss after mini-batch   100: 2.812\n",
      "Loss after mini-batch   150: 2.854\n",
      "Loss after mini-batch   200: 2.752\n",
      "Starting epoch 30\n",
      "Loss after mini-batch    50: 2.710\n",
      "Loss after mini-batch   100: 2.741\n",
      "Loss after mini-batch   150: 2.652\n",
      "Loss after mini-batch   200: 2.625\n",
      "Loss after mini-batch    50: 2.634\n",
      "Loss after mini-batch   100: 2.751\n",
      "Loss after mini-batch   150: 2.823\n",
      "Loss after mini-batch   200: 2.660\n",
      "Loss after mini-batch    50: 2.685\n",
      "Loss after mini-batch   100: 2.738\n",
      "Loss after mini-batch   150: 2.802\n",
      "Loss after mini-batch   200: 2.753\n",
      "Loss after mini-batch    50: 2.815\n",
      "Loss after mini-batch   100: 2.570\n",
      "Loss after mini-batch   150: 2.696\n",
      "Loss after mini-batch   200: 2.747\n",
      "Loss after mini-batch    50: 2.722\n",
      "Loss after mini-batch   100: 2.697\n",
      "Loss after mini-batch   150: 2.806\n",
      "Loss after mini-batch   200: 2.593\n",
      "Loss after mini-batch    50: 2.838\n",
      "Loss after mini-batch   100: 2.726\n",
      "Loss after mini-batch   150: 2.719\n",
      "Loss after mini-batch   200: 2.657\n",
      "Loss after mini-batch    50: 2.635\n",
      "Loss after mini-batch   100: 2.825\n",
      "Loss after mini-batch   150: 2.800\n",
      "Loss after mini-batch   200: 2.733\n",
      "Loss after mini-batch    50: 2.767\n",
      "Loss after mini-batch   100: 2.880\n",
      "Loss after mini-batch   150: 2.756\n",
      "Loss after mini-batch   200: 2.822\n",
      "Loss after mini-batch    50: 2.857\n",
      "Loss after mini-batch   100: 2.654\n",
      "Loss after mini-batch   150: 2.704\n",
      "Loss after mini-batch   200: 2.658\n",
      "Loss after mini-batch    50: 2.619\n",
      "Loss after mini-batch   100: 2.768\n",
      "Loss after mini-batch   150: 2.648\n",
      "Loss after mini-batch   200: 2.706\n",
      "Loss after mini-batch    50: 2.720\n",
      "Loss after mini-batch   100: 2.725\n",
      "Loss after mini-batch   150: 2.674\n",
      "Loss after mini-batch   200: 2.697\n",
      "Loss after mini-batch    50: 2.604\n",
      "Loss after mini-batch   100: 2.719\n",
      "Loss after mini-batch   150: 2.526\n",
      "Loss after mini-batch   200: 2.786\n",
      "Loss after mini-batch    50: 2.649\n",
      "Loss after mini-batch   100: 2.603\n",
      "Loss after mini-batch   150: 2.717\n",
      "Loss after mini-batch   200: 2.782\n",
      "Loss after mini-batch    50: 2.798\n",
      "Loss after mini-batch   100: 2.770\n",
      "Loss after mini-batch   150: 2.806\n",
      "Loss after mini-batch   200: 2.636\n",
      "Loss after mini-batch    50: 2.824\n",
      "Loss after mini-batch   100: 2.921\n",
      "Loss after mini-batch   150: 2.814\n",
      "Loss after mini-batch   200: 2.765\n",
      "Starting epoch 31\n",
      "Loss after mini-batch    50: 2.657\n",
      "Loss after mini-batch   100: 2.733\n",
      "Loss after mini-batch   150: 2.667\n",
      "Loss after mini-batch   200: 2.672\n",
      "Loss after mini-batch    50: 2.743\n",
      "Loss after mini-batch   100: 2.753\n",
      "Loss after mini-batch   150: 2.726\n",
      "Loss after mini-batch   200: 2.819\n",
      "Loss after mini-batch    50: 2.687\n",
      "Loss after mini-batch   100: 2.773\n",
      "Loss after mini-batch   150: 2.904\n",
      "Loss after mini-batch   200: 2.556\n",
      "Loss after mini-batch    50: 2.682\n",
      "Loss after mini-batch   100: 2.672\n",
      "Loss after mini-batch   150: 2.813\n",
      "Loss after mini-batch   200: 2.708\n",
      "Loss after mini-batch    50: 2.626\n",
      "Loss after mini-batch   100: 2.665\n",
      "Loss after mini-batch   150: 2.778\n",
      "Loss after mini-batch   200: 2.770\n",
      "Loss after mini-batch    50: 2.668\n",
      "Loss after mini-batch   100: 2.693\n",
      "Loss after mini-batch   150: 2.690\n",
      "Loss after mini-batch   200: 2.722\n",
      "Loss after mini-batch    50: 2.710\n",
      "Loss after mini-batch   100: 2.834\n",
      "Loss after mini-batch   150: 2.708\n",
      "Loss after mini-batch   200: 2.661\n",
      "Loss after mini-batch    50: 2.669\n",
      "Loss after mini-batch   100: 2.609\n",
      "Loss after mini-batch   150: 2.846\n",
      "Loss after mini-batch   200: 2.794\n",
      "Loss after mini-batch    50: 2.546\n",
      "Loss after mini-batch   100: 2.725\n",
      "Loss after mini-batch   150: 2.742\n",
      "Loss after mini-batch   200: 2.742\n",
      "Loss after mini-batch    50: 2.730\n",
      "Loss after mini-batch   100: 2.780\n",
      "Loss after mini-batch   150: 2.567\n",
      "Loss after mini-batch   200: 2.783\n",
      "Loss after mini-batch    50: 2.765\n",
      "Loss after mini-batch   100: 2.767\n",
      "Loss after mini-batch   150: 2.592\n",
      "Loss after mini-batch   200: 2.743\n",
      "Loss after mini-batch    50: 2.668\n",
      "Loss after mini-batch   100: 2.671\n",
      "Loss after mini-batch   150: 2.618\n",
      "Loss after mini-batch   200: 2.759\n",
      "Loss after mini-batch    50: 2.598\n",
      "Loss after mini-batch   100: 2.542\n",
      "Loss after mini-batch   150: 2.744\n",
      "Loss after mini-batch   200: 2.712\n",
      "Loss after mini-batch    50: 2.639\n",
      "Loss after mini-batch   100: 2.713\n",
      "Loss after mini-batch   150: 2.706\n",
      "Loss after mini-batch   200: 2.836\n",
      "Loss after mini-batch    50: 3.056\n",
      "Loss after mini-batch   100: 2.821\n",
      "Loss after mini-batch   150: 2.663\n",
      "Loss after mini-batch   200: 2.820\n",
      "Starting epoch 32\n",
      "Loss after mini-batch    50: 2.721\n",
      "Loss after mini-batch   100: 2.762\n",
      "Loss after mini-batch   150: 2.752\n",
      "Loss after mini-batch   200: 2.591\n",
      "Loss after mini-batch    50: 2.601\n",
      "Loss after mini-batch   100: 2.727\n",
      "Loss after mini-batch   150: 2.853\n",
      "Loss after mini-batch   200: 2.603\n",
      "Loss after mini-batch    50: 2.675\n",
      "Loss after mini-batch   100: 2.735\n",
      "Loss after mini-batch   150: 2.698\n",
      "Loss after mini-batch   200: 2.791\n",
      "Loss after mini-batch    50: 2.657\n",
      "Loss after mini-batch   100: 2.818\n",
      "Loss after mini-batch   150: 2.674\n",
      "Loss after mini-batch   200: 2.748\n",
      "Loss after mini-batch    50: 2.744\n",
      "Loss after mini-batch   100: 2.690\n",
      "Loss after mini-batch   150: 2.875\n",
      "Loss after mini-batch   200: 2.637\n",
      "Loss after mini-batch    50: 2.862\n",
      "Loss after mini-batch   100: 2.669\n",
      "Loss after mini-batch   150: 2.732\n",
      "Loss after mini-batch   200: 2.654\n",
      "Loss after mini-batch    50: 2.719\n",
      "Loss after mini-batch   100: 2.692\n",
      "Loss after mini-batch   150: 2.730\n",
      "Loss after mini-batch   200: 2.821\n",
      "Loss after mini-batch    50: 2.669\n",
      "Loss after mini-batch   100: 2.774\n",
      "Loss after mini-batch   150: 2.858\n",
      "Loss after mini-batch   200: 2.743\n",
      "Loss after mini-batch    50: 2.865\n",
      "Loss after mini-batch   100: 2.726\n",
      "Loss after mini-batch   150: 2.682\n",
      "Loss after mini-batch   200: 2.628\n",
      "Loss after mini-batch    50: 2.638\n",
      "Loss after mini-batch   100: 2.741\n",
      "Loss after mini-batch   150: 2.631\n",
      "Loss after mini-batch   200: 2.599\n",
      "Loss after mini-batch    50: 2.723\n",
      "Loss after mini-batch   100: 2.747\n",
      "Loss after mini-batch   150: 2.830\n",
      "Loss after mini-batch   200: 2.629\n",
      "Loss after mini-batch    50: 2.686\n",
      "Loss after mini-batch   100: 2.727\n",
      "Loss after mini-batch   150: 2.766\n",
      "Loss after mini-batch   200: 2.599\n",
      "Loss after mini-batch    50: 2.699\n",
      "Loss after mini-batch   100: 2.792\n",
      "Loss after mini-batch   150: 2.670\n",
      "Loss after mini-batch   200: 2.695\n",
      "Loss after mini-batch    50: 2.731\n",
      "Loss after mini-batch   100: 2.759\n",
      "Loss after mini-batch   150: 2.631\n",
      "Loss after mini-batch   200: 2.681\n",
      "Loss after mini-batch    50: 2.657\n",
      "Loss after mini-batch   100: 2.839\n",
      "Loss after mini-batch   150: 2.777\n",
      "Loss after mini-batch   200: 2.973\n",
      "Starting epoch 33\n",
      "Loss after mini-batch    50: 2.699\n",
      "Loss after mini-batch   100: 2.643\n",
      "Loss after mini-batch   150: 2.702\n",
      "Loss after mini-batch   200: 2.739\n",
      "Loss after mini-batch    50: 2.814\n",
      "Loss after mini-batch   100: 2.680\n",
      "Loss after mini-batch   150: 2.689\n",
      "Loss after mini-batch   200: 2.747\n",
      "Loss after mini-batch    50: 2.796\n",
      "Loss after mini-batch   100: 2.797\n",
      "Loss after mini-batch   150: 2.591\n",
      "Loss after mini-batch   200: 2.746\n",
      "Loss after mini-batch    50: 2.778\n",
      "Loss after mini-batch   100: 2.679\n",
      "Loss after mini-batch   150: 2.653\n",
      "Loss after mini-batch   200: 2.746\n",
      "Loss after mini-batch    50: 2.751\n",
      "Loss after mini-batch   100: 2.765\n",
      "Loss after mini-batch   150: 2.611\n",
      "Loss after mini-batch   200: 2.743\n",
      "Loss after mini-batch    50: 2.582\n",
      "Loss after mini-batch   100: 2.762\n",
      "Loss after mini-batch   150: 2.766\n",
      "Loss after mini-batch   200: 2.734\n",
      "Loss after mini-batch    50: 2.748\n",
      "Loss after mini-batch   100: 2.735\n",
      "Loss after mini-batch   150: 2.709\n",
      "Loss after mini-batch   200: 2.718\n",
      "Loss after mini-batch    50: 2.758\n",
      "Loss after mini-batch   100: 2.841\n",
      "Loss after mini-batch   150: 2.863\n",
      "Loss after mini-batch   200: 2.581\n",
      "Loss after mini-batch    50: 2.807\n",
      "Loss after mini-batch   100: 2.836\n",
      "Loss after mini-batch   150: 2.703\n",
      "Loss after mini-batch   200: 2.563\n",
      "Loss after mini-batch    50: 2.681\n",
      "Loss after mini-batch   100: 2.696\n",
      "Loss after mini-batch   150: 2.664\n",
      "Loss after mini-batch   200: 2.664\n",
      "Loss after mini-batch    50: 2.921\n",
      "Loss after mini-batch   100: 2.672\n",
      "Loss after mini-batch   150: 2.675\n",
      "Loss after mini-batch   200: 2.634\n",
      "Loss after mini-batch    50: 2.664\n",
      "Loss after mini-batch   100: 2.772\n",
      "Loss after mini-batch   150: 2.672\n",
      "Loss after mini-batch   200: 2.547\n",
      "Loss after mini-batch    50: 2.596\n",
      "Loss after mini-batch   100: 2.800\n",
      "Loss after mini-batch   150: 2.673\n",
      "Loss after mini-batch   200: 2.677\n",
      "Loss after mini-batch    50: 2.661\n",
      "Loss after mini-batch   100: 2.746\n",
      "Loss after mini-batch   150: 2.660\n",
      "Loss after mini-batch   200: 2.851\n",
      "Loss after mini-batch    50: 2.773\n",
      "Loss after mini-batch   100: 2.643\n",
      "Loss after mini-batch   150: 2.860\n",
      "Loss after mini-batch   200: 2.945\n",
      "Starting epoch 34\n",
      "Loss after mini-batch    50: 2.804\n",
      "Loss after mini-batch   100: 2.583\n",
      "Loss after mini-batch   150: 2.564\n",
      "Loss after mini-batch   200: 2.735\n",
      "Loss after mini-batch    50: 2.782\n",
      "Loss after mini-batch   100: 2.776\n",
      "Loss after mini-batch   150: 2.638\n",
      "Loss after mini-batch   200: 2.784\n",
      "Loss after mini-batch    50: 2.750\n",
      "Loss after mini-batch   100: 2.722\n",
      "Loss after mini-batch   150: 2.690\n",
      "Loss after mini-batch   200: 2.706\n",
      "Loss after mini-batch    50: 2.817\n",
      "Loss after mini-batch   100: 2.716\n",
      "Loss after mini-batch   150: 2.760\n",
      "Loss after mini-batch   200: 2.645\n",
      "Loss after mini-batch    50: 2.820\n",
      "Loss after mini-batch   100: 2.681\n",
      "Loss after mini-batch   150: 2.685\n",
      "Loss after mini-batch   200: 2.695\n",
      "Loss after mini-batch    50: 2.662\n",
      "Loss after mini-batch   100: 2.788\n",
      "Loss after mini-batch   150: 2.733\n",
      "Loss after mini-batch   200: 2.693\n",
      "Loss after mini-batch    50: 2.746\n",
      "Loss after mini-batch   100: 2.577\n",
      "Loss after mini-batch   150: 2.844\n",
      "Loss after mini-batch   200: 2.894\n",
      "Loss after mini-batch    50: 2.761\n",
      "Loss after mini-batch   100: 2.852\n",
      "Loss after mini-batch   150: 2.732\n",
      "Loss after mini-batch   200: 2.772\n",
      "Loss after mini-batch    50: 2.703\n",
      "Loss after mini-batch   100: 2.604\n",
      "Loss after mini-batch   150: 2.812\n",
      "Loss after mini-batch   200: 2.644\n",
      "Loss after mini-batch    50: 2.707\n",
      "Loss after mini-batch   100: 2.486\n",
      "Loss after mini-batch   150: 2.717\n",
      "Loss after mini-batch   200: 2.737\n",
      "Loss after mini-batch    50: 2.717\n",
      "Loss after mini-batch   100: 2.669\n",
      "Loss after mini-batch   150: 2.827\n",
      "Loss after mini-batch   200: 2.612\n",
      "Loss after mini-batch    50: 2.744\n",
      "Loss after mini-batch   100: 2.681\n",
      "Loss after mini-batch   150: 2.673\n",
      "Loss after mini-batch   200: 2.617\n",
      "Loss after mini-batch    50: 2.538\n",
      "Loss after mini-batch   100: 2.661\n",
      "Loss after mini-batch   150: 2.766\n",
      "Loss after mini-batch   200: 2.790\n",
      "Loss after mini-batch    50: 2.733\n",
      "Loss after mini-batch   100: 2.806\n",
      "Loss after mini-batch   150: 2.721\n",
      "Loss after mini-batch   200: 2.813\n",
      "Loss after mini-batch    50: 2.752\n",
      "Loss after mini-batch   100: 2.865\n",
      "Loss after mini-batch   150: 2.880\n",
      "Loss after mini-batch   200: 2.826\n",
      "Starting epoch 35\n",
      "Loss after mini-batch    50: 2.750\n",
      "Loss after mini-batch   100: 2.793\n",
      "Loss after mini-batch   150: 2.591\n",
      "Loss after mini-batch   200: 2.633\n",
      "Loss after mini-batch    50: 2.843\n",
      "Loss after mini-batch   100: 2.707\n",
      "Loss after mini-batch   150: 2.638\n",
      "Loss after mini-batch   200: 2.814\n",
      "Loss after mini-batch    50: 2.675\n",
      "Loss after mini-batch   100: 2.715\n",
      "Loss after mini-batch   150: 2.742\n",
      "Loss after mini-batch   200: 2.634\n",
      "Loss after mini-batch    50: 2.628\n",
      "Loss after mini-batch   100: 2.775\n",
      "Loss after mini-batch   150: 2.821\n",
      "Loss after mini-batch   200: 2.681\n",
      "Loss after mini-batch    50: 2.661\n",
      "Loss after mini-batch   100: 2.804\n",
      "Loss after mini-batch   150: 2.623\n",
      "Loss after mini-batch   200: 2.748\n",
      "Loss after mini-batch    50: 2.681\n",
      "Loss after mini-batch   100: 2.605\n",
      "Loss after mini-batch   150: 2.721\n",
      "Loss after mini-batch   200: 2.905\n",
      "Loss after mini-batch    50: 2.691\n",
      "Loss after mini-batch   100: 2.748\n",
      "Loss after mini-batch   150: 2.827\n",
      "Loss after mini-batch   200: 2.781\n",
      "Loss after mini-batch    50: 2.693\n",
      "Loss after mini-batch   100: 2.862\n",
      "Loss after mini-batch   150: 2.679\n",
      "Loss after mini-batch   200: 2.729\n",
      "Loss after mini-batch    50: 2.632\n",
      "Loss after mini-batch   100: 2.723\n",
      "Loss after mini-batch   150: 2.672\n",
      "Loss after mini-batch   200: 2.601\n",
      "Loss after mini-batch    50: 2.683\n",
      "Loss after mini-batch   100: 2.650\n",
      "Loss after mini-batch   150: 2.681\n",
      "Loss after mini-batch   200: 2.665\n",
      "Loss after mini-batch    50: 2.737\n",
      "Loss after mini-batch   100: 2.553\n",
      "Loss after mini-batch   150: 2.739\n",
      "Loss after mini-batch   200: 2.796\n",
      "Loss after mini-batch    50: 2.614\n",
      "Loss after mini-batch   100: 2.633\n",
      "Loss after mini-batch   150: 2.694\n",
      "Loss after mini-batch   200: 2.696\n",
      "Loss after mini-batch    50: 2.717\n",
      "Loss after mini-batch   100: 2.670\n",
      "Loss after mini-batch   150: 2.675\n",
      "Loss after mini-batch   200: 2.694\n",
      "Loss after mini-batch    50: 2.605\n",
      "Loss after mini-batch   100: 2.822\n",
      "Loss after mini-batch   150: 2.837\n",
      "Loss after mini-batch   200: 2.663\n",
      "Loss after mini-batch    50: 2.767\n",
      "Loss after mini-batch   100: 2.775\n",
      "Loss after mini-batch   150: 2.844\n",
      "Loss after mini-batch   200: 2.851\n",
      "Starting epoch 36\n",
      "Loss after mini-batch    50: 2.653\n",
      "Loss after mini-batch   100: 2.641\n",
      "Loss after mini-batch   150: 2.788\n",
      "Loss after mini-batch   200: 2.625\n",
      "Loss after mini-batch    50: 2.750\n",
      "Loss after mini-batch   100: 2.715\n",
      "Loss after mini-batch   150: 2.781\n",
      "Loss after mini-batch   200: 2.736\n",
      "Loss after mini-batch    50: 2.685\n",
      "Loss after mini-batch   100: 2.682\n",
      "Loss after mini-batch   150: 2.715\n",
      "Loss after mini-batch   200: 2.633\n",
      "Loss after mini-batch    50: 2.577\n",
      "Loss after mini-batch   100: 2.822\n",
      "Loss after mini-batch   150: 2.911\n",
      "Loss after mini-batch   200: 2.600\n",
      "Loss after mini-batch    50: 2.728\n",
      "Loss after mini-batch   100: 2.729\n",
      "Loss after mini-batch   150: 2.729\n",
      "Loss after mini-batch   200: 2.654\n",
      "Loss after mini-batch    50: 2.807\n",
      "Loss after mini-batch   100: 2.648\n",
      "Loss after mini-batch   150: 2.788\n",
      "Loss after mini-batch   200: 2.668\n",
      "Loss after mini-batch    50: 2.726\n",
      "Loss after mini-batch   100: 2.601\n",
      "Loss after mini-batch   150: 2.796\n",
      "Loss after mini-batch   200: 2.886\n",
      "Loss after mini-batch    50: 2.611\n",
      "Loss after mini-batch   100: 2.825\n",
      "Loss after mini-batch   150: 2.775\n",
      "Loss after mini-batch   200: 2.852\n",
      "Loss after mini-batch    50: 2.661\n",
      "Loss after mini-batch   100: 2.731\n",
      "Loss after mini-batch   150: 2.657\n",
      "Loss after mini-batch   200: 2.646\n",
      "Loss after mini-batch    50: 2.647\n",
      "Loss after mini-batch   100: 2.723\n",
      "Loss after mini-batch   150: 2.711\n",
      "Loss after mini-batch   200: 2.603\n",
      "Loss after mini-batch    50: 2.786\n",
      "Loss after mini-batch   100: 2.719\n",
      "Loss after mini-batch   150: 2.747\n",
      "Loss after mini-batch   200: 2.720\n",
      "Loss after mini-batch    50: 2.758\n",
      "Loss after mini-batch   100: 2.701\n",
      "Loss after mini-batch   150: 2.700\n",
      "Loss after mini-batch   200: 2.518\n",
      "Loss after mini-batch    50: 2.717\n",
      "Loss after mini-batch   100: 2.564\n",
      "Loss after mini-batch   150: 2.797\n",
      "Loss after mini-batch   200: 2.734\n",
      "Loss after mini-batch    50: 2.639\n",
      "Loss after mini-batch   100: 2.789\n",
      "Loss after mini-batch   150: 2.797\n",
      "Loss after mini-batch   200: 2.548\n",
      "Loss after mini-batch    50: 2.733\n",
      "Loss after mini-batch   100: 2.840\n",
      "Loss after mini-batch   150: 2.822\n",
      "Loss after mini-batch   200: 2.873\n",
      "Starting epoch 37\n",
      "Loss after mini-batch    50: 2.681\n",
      "Loss after mini-batch   100: 2.581\n",
      "Loss after mini-batch   150: 2.793\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8dd2228bbfc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m       \u001b[0;31m#print(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0;31m# Perform backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m       \u001b[0;31m# Perform optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set fixed random number seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Initialize the MLP\n",
    "mlp = MLP()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "#loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
    "\n",
    "sftmx = nn.Softmax(dim=1)\n",
    "\n",
    "# Run the training loop\n",
    "for epoch in range(0, 50): # 5 epochs at maximum\n",
    "  # Print epoch\n",
    "  print(f'Starting epoch {epoch+1}')\n",
    "  chunk_size = 500000\n",
    "  train_filename = \"/content/drive/MyDrive/Colab_Notebooks/Ebay/data/ebay_train.tsv.gz\"\n",
    "  out_filename = \"/content/drive/MyDrive/Colab_Notebooks/Ebay/data/ebay_train_shuffle.tsv.gz\"\n",
    "\n",
    "  shuffle_train_set(train_filename, out_filename)\n",
    "\n",
    "  for chunk in pd.read_csv(out_filename, sep='\\t', chunksize=chunk_size):\n",
    "    #process(chunk)\n",
    "    dataset = EbayDataset(chunk,train_size=chunk_size)\n",
    "    # Prepare dataset\n",
    "    trainloader = torch.utils.data.DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=1)\n",
    "\n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(trainloader):\n",
    "      mlp.train()\n",
    "      # Get inputs\n",
    "      inputs, targets = data\n",
    "      \n",
    "      # Zero the gradients\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      # Perform forward pass\n",
    "      outputs = mlp(inputs.float())\n",
    "      \n",
    "      # Compute loss\n",
    "      loss = loss_function(outputs, torch.reshape(targets.float(),(len(outputs),1)))\n",
    "      #print(loss)\n",
    "      # Perform backward pass\n",
    "      loss.backward()\n",
    "      \n",
    "      # Perform optimization\n",
    "      optimizer.step()\n",
    "      \n",
    "      # Print statistics\n",
    "      current_loss += loss.item()\n",
    "      if i % 50 == 49:\n",
    "        print('Loss after mini-batch %5d: %.3f' %\n",
    "                (i + 1, current_loss / 50))\n",
    "          \n",
    "        with torch.no_grad():\n",
    "          test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=300000, shuffle=True, num_workers=1)\n",
    "          inputs_all, targets_all = next(iter(test_loader))\n",
    "          outputs_all = mlp(inputs_all.float())\n",
    "\n",
    "          y_pred = sftmx(outputs_all.detach()) #np.round(outputs.detach().numpy())\n",
    "          y_pred = np.argmax(y_pred, axis=1)\n",
    "          acc = accuracy_score(targets_all, y_pred)\n",
    "          print(\"Test accuracy:\", acc)\n",
    "\n",
    "        current_loss = 0.0\n",
    "\n",
    "  torch.save(mlp.state_dict(), \"/content/drive/MyDrive/Colab_Notebooks/Ebay/data/mlp3/mlp_mid_train\"+str(epoch)+\"_epoch.pt\")\n",
    "\n",
    "# Process is complete.\n",
    "print('Training process has finished.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wsjZivWd7iE7"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# evaluate model:\n",
    "mlp.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  all_dataloader = torch.utils.data.DataLoader(dataset, batch_size=train_set_size, shuffle=True, num_workers=1)\n",
    "  inputs_all, targets_all = next(iter(all_dataloader))\n",
    "  outputs_all = mlp(inputs_all.float())\n",
    "  \n",
    "  y_pred = outputs_all.detach()\n",
    "  y_pred = np.round(y_pred.numpy().reshape(len(y_pred)))\n",
    "  diff_all = y_pred - targets_all.numpy()\n",
    "  diff_sq = diff_all*diff_all\n",
    "  #y_pred = sftmx(outputs_all.detach())\n",
    "  #y_pred = np.argmax(y_pred, axis=1)\n",
    "  acc = np.mean(diff_sq)\n",
    "  print(\"Train Loss:\", acc)\n",
    "  #acc = accuracy_score(targets_all, y_pred)\n",
    "  #print(\"Train Acc:\", acc)\n",
    "  \n",
    "  test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=300000, shuffle=True, num_workers=1)\n",
    "  inputs_all, targets_all = next(iter(test_loader))\n",
    "  outputs_all = mlp(inputs_all.float())\n",
    "\n",
    "  y_pred = sftmx(outputs_all.detach()) #np.round(outputs.detach().numpy())\n",
    "  y_pred = np.argmax(y_pred, axis=1)\n",
    "  acc = accuracy_score(targets_all, y_pred)\n",
    "  print(\"Test accuracy:\", acc)\n",
    "  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nniyhFnRhAZ3"
   },
   "outputs": [],
   "source": [
    "#y_pred.numpy().reshape(len(y_pred))\n",
    "outputs\n",
    "#y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LdMPtcdbAj0A"
   },
   "outputs": [],
   "source": [
    "\n",
    "#torch.save(mlp.state_dict(), \"/content/drive/MyDrive/Colab_Notebooks/Ebay/data/mlp_2_epoch.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VSFmP2ySqzh2",
    "outputId": "e1eeac6e-0393-4d01-e5a0-7e2b315319e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (w1): Linear(in_features=15, out_features=32, bias=True)\n",
       "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (w2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (w3): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP()\n",
    "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab_Notebooks/Ebay/data/mlp2/mlp_mid_train9_epoch.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvR0-ZW7ym2O"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "dataset_quiz = EbayDataset(\"/content/drive/MyDrive/Colab_Notebooks/Ebay/data/eBay_ML_Challenge_Dataset_2021_quiz.tsv.gz\", train_size=2500000, quiz_set=1)\n",
    "#dataset_quiz = EbayDataset(\"/content/drive/MyDrive/Colab_Notebooks/Ebay/data/eBay_ML_Challenge_Dataset_2021_train.tsv.gz\", train_size=2500000)\n",
    "\n",
    "quiz_loader = torch.utils.data.DataLoader(dataset_quiz, batch_size=2500000, shuffle=False, num_workers=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CauBEmw19SQl"
   },
   "outputs": [],
   "source": [
    "inputs_all, targets_all = next(iter(quiz_loader))\n",
    "outputs_all = model(inputs_all.float())\n",
    "\n",
    "#y_pred = sftmx(outputs_all.detach()) #np.round(outputs.detach().numpy())\n",
    "#y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "y_pred = outputs_all.detach()\n",
    "y_pred = np.round(y_pred.numpy().reshape(len(y_pred)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IwXFmaxb0EoI",
    "outputId": "f11a2a5b-08be-4b50-eaf7-6f3fb160d806"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 5., 4., ..., 3., 6., 4.], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Vt-foXKivBQ"
   },
   "outputs": [],
   "source": [
    "#y_pred_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PdHdefUjKv-u"
   },
   "outputs": [],
   "source": [
    "dataset_quiz = None\n",
    "quiz_loader = None\n",
    "inputs_all = None\n",
    "outputs_all = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zxn4YUz3Ij7X"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def get_delivery_date_quiz(row):\n",
    "  \n",
    "  payment_date = datetime.datetime.fromisoformat(row['payment_datetime'][:10])\n",
    "  #print(payment_date)\n",
    "  #delivery_date = datetime.fromisoformat(row['delivery_date'])\n",
    "  delivery_date = payment_date + datetime.timedelta(days=row['mlp_pred'])\n",
    "  return delivery_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tnJ1r1iAIJPI"
   },
   "outputs": [],
   "source": [
    "quiz_df = pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/Ebay/data/eBay_ML_Challenge_Dataset_2021_quiz.tsv.gz\", sep=\"\\t\")\n",
    "quiz_df['mlp_pred'] = y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBqSbH9bMETg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bEttpNn9sPy"
   },
   "outputs": [],
   "source": [
    "quiz_df['mlp_pred'] = quiz_df.apply(get_delivery_date_quiz, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Ux6GxNd9sKI"
   },
   "outputs": [],
   "source": [
    "quiz_df[['record_number', 'mlp_pred']].to_csv('/content/drive/MyDrive/Colab_Notebooks/Ebay/data/eBay_ML_Challenge_Dataset_2021_quiz_mlp_pred.tsv.gz',sep='\\t',header=False, index=False, compression='infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9wcRTFI9sFv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgkH96Rb9r9z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQxUJYfRIJGR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnsaBQJeIJCk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SR_qcn_oN9a1",
    "outputId": "62d74c2b-bcf9-4ff8-8ad8-d1a334e67575"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f22191a2550>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ")it = iter(trainloader)\n",
    "#first = next(it)\n",
    "#first\n",
    "trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "527dgfXWxnN1"
   },
   "outputs": [],
   "source": [
    "linear regression\n",
    "xgboost regresser\n",
    "#https://www.datatechnotes.com/2019/06/regression-example-with-xgbregressor-in.html\n",
    "decision tree regressor\n",
    "random forest\n",
    "ENSEMBLE - diff things for diff features\n",
    "\n",
    "remove outliers\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of mlp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ebay-ml",
   "language": "python",
   "name": "ebay-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
